\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[linguistics]{forest}
\usepackage{mathtools}
\usepackage{bbm}
\usepackage[utf8]{inputenc} % this is needed for umlauts
\usepackage[ngerman]{babel} % this is needed for umlauts
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage[makeroom]{cancel}
\usepackage{tikz}
\usepackage{enumitem}
\usepackage{ulem}

\expandafter\def\expandafter\normalsize\expandafter{%
    \normalsize
    \setlength\abovedisplayskip{20pt}
    \setlength\belowdisplayskip{15pt}
    \setlength\abovedisplayshortskip{15pt}
    \setlength\belowdisplayshortskip{13pt}
}

\input ulem.sty

\newlist{arrowlist}{itemize}{1}
\setlist[arrowlist]{label=$\Rightarrow$}

\tikzset{node distance = 1cm and 5cm}
\usetikzlibrary{trees}
\usepackage[a4paper,bindingoffset=0.2in,
            left=1in,right=1in,top=1.2in,bottom=1in,
            footskip=.25in]{geometry}

\pagestyle{fancy}
\fancyhf{}
\rhead{\thesubsection}
\lhead{Fundamentals of Machine Learning}
\rfoot{Page \thepage}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclareMathOperator{\rank}{rank}


% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            %
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}



\title{Fundamentels of Machine Learning}
\author{Ullrich Köthe}
\date{WS 2017, Heidelberg}

\begin{document}
  \maketitle
  \vspace{25mm}
  \begin{center}
    Skript zur Vorlesung an der Universität Heidelberg
  \end{center}
  \newpage

  \tableofcontents
  \newpage
  \section*{Rational of Machine Learning}
    \begin{itemize}
      \item Interest in attributes/quantities Y ("response"), but they are not easily measurable.
      \item Choose attributes/quantities X ("features"), that are easy to measure.
      \item find a mapping $Y = f(X)$ to determine Y indirectly
      \item many problems don't have an explicit analytical $f(X)$
      \item $\Rightarrow$ use "generic mapping": $Y = f(X, \theta)$, $\theta:$ adjustable parameters (ideally a universal approximate) and adjust $\theta$ by learning from a training set:
      \begin{equation*}
        TS = \{(X_{i}, Y_{i})\}_{i=1}^{N}
      \end{equation*}
      \item usually the relation between X and Y is not deterministic
      \item posterior probability:
    \end{itemize}
    \begin{equation*}
      p(Y|X;\theta)
    \end{equation*}

    \begin{equation*}
    f(X) = \begin{cases}
    Y_{1} &\text{with probability $p(Y=Y_{1}|X;\theta)$}\\
    Y_{2} &\text{with probability $p(Y=Y_{2}|X;\theta)$}
    \end{cases}
    \end{equation*}

  \section*{Kinds of Variables}
    \begin{itemize}
      \item numeric: $X \in \mathbb{R}^D, Y \in \mathbb{R}^M$ (usually M=1)
      \item discrete: $X \in \{ A, B, C,... \}$
      \item ordinal: categories are ordered $A < B < C$
      \item categorical: $X \in \{"red", "green", "blue" \}, Y \in \{"pea", "pear", "peach" \}$
    \end{itemize}
    if response is discrete $\Rightarrow$ classification \\
    if response is numeric $\Rightarrow$ regression
    \subsection*{Notation}
    \begin{itemize}
      \item instances in training set subscript $i \in \{ 1,..., N\}$
      \item $X_{i}:$ features of training instance (row vector)
      \item$Y_{i}:$ response
      \item features we measure: subscript $j \in \{1,...,M\}$
      \item $X_{j}:$ j-th feature of all instances (column vector)
      \item $X_{ij}:$ j-th feature of instance i (a scalar)
    \end{itemize}
  \section*{Kinds of training data}
    \begin{itemize}
      \item supervised learning: $Y_{i}$ is known for all training instances
        \begin{itemize}
          \item possible if we can obtain $Y_{i}$ in a research setting
          \item measuring, asking expert
          \item measuring $Y_{i}$ is destructive (crash test)
          \item $Y_{i}$ is only known in hind sight
          \item we know the mapping $Y_{i} = f(Y_{i})$ for $TS\{(X_{i}, Y_{i})\}$
          \item training strategies:
          \begin{itemize}
            \item watch training: TS is given beforehand
            \item online training
            \item active training
          \end{itemize}
        \end{itemize}
        \item unsupervised learning: $X_{i}$ are known, $Y_{i}$ not ("data mining")
        \begin{itemize}
          \item group data by similarity
          \item determine useful categories
          \item find interesting features
          \item estimate probability distribution $p(X)$
          \item novely detection - find unusual X
        \end{itemize}
    \end{itemize}
  \section{Classification}
    \subsection{Rules}
      \begin{itemize}
        \item instances are i.i.d. (independent identically distributed)
        \item Y is discrete with C categories $Y \in \{ 1, 2,..., C\}$ \\
          $C = 2: Y \in \{ 0, 1\}, \{ -1, 1\}$
        \item X is numeric or discrete
        \item if the outcome is certain: estimate posterior probability $p(Y|X;\theta)$
        \item but in many situations a hard decision is needed:
      \end{itemize}
      Example: hiring X, credentials: $p(Y=\text{will design good bike}|X)$
      needs a hard decision function:
      \begin{equation*}
        f(x) =
        \begin{cases}
          \text{hire if X is convincing}\\
          \text{not hire else}
        \end{cases}
      \end{equation*}
    \subsection{How badly does a bad decision function perform?}
      Suppose we know the prior probability of each category (prior - before,
      without measuring X)
      \begin{equation*}
        p(Y=K) = \pi_{k} \quad k = 1,...,C
      \end{equation*}
      \begin{equation*}
        \Rightarrow \text{most sensible decision function:} \quad f(k) = \argmax_{k} \pi_{k} = \hat{k}
      \end{equation*}
      \begin{equation*}
        \text{success rate} = \pi_{\hat{k}}
      \end{equation*}
      \begin{equation*}
        \text{error rate} = 1 - \pi_{\hat{k}} = 1 - \argmax_{k} \pi_{k}
      \end{equation*}
    \subsection{How good can a decision function perform in an uncertain environment?}
      Sources of uncertainty:
      \begin{itemize}
        \item intrinsic uncertainty:
        \begin{itemize}
          \item fundamental(Q.M.)
          \item noise (can measure X and Y only to certain accuracy)
        \end{itemize}
        \item insufficient knowledge:
        \begin{itemize}
          \item X may not have enough information to determine Y exactly
          \item missing data
        \end{itemize}
        \item modelling uncertainty:
        \begin{itemize}
          \item $Y = f(X, \theta)$ may not be powered enough to express the true
          relationship $Y = f^*(X)$
          \item $\theta$ may not be set to the optimal values
        \end{itemize}
      \end{itemize}
      Assume that there is no modelling error and no noise in Y, no missing data
      \begin{equation*}
        \Rightarrow \text{our posterior:} \quad \hat{p}(Y|X) = p^*(Y|X)
      \end{equation*}
      so we know the probability distribution. \\
      What decision funtion $\hat{f}$ minimizes the error fo C = 2?
      \begin{equation*}
        p^*(Y=1|X) \quad p^*(Y=-1|X)
      \end{equation*}
      \begin{equation*}
        \text{Two decision functions:}
        \begin{cases}
        f_{1}(X) = f(X;\theta_{1}) = 1 \\
        f_{-1}(X) = f(X;\theta_{-1}) = -1
        \end{cases}
      \end{equation*}
      choose $f_{1}$:
      \begin{itemize}
        \item a) true positive $Y^*(X) = 1$
        \item b) false positive $Y^*(X) = -1$
      \end{itemize}
      choose $f_{-1}$:
      \begin{itemize}
        \item c) true negative $Y^*(X) = -1$
        \item d) false negative $Y^*(X) = 1$
      \end{itemize}
      Probabilities: a) = d):\quad $p^*(Y=-1|X) = 1 - p^*(Y=1|X)$ \\
      Success is maximized if we always decide fo most probable outcome, given X:
      \begin{equation*}
        \hat{Y} = \hat{f}(X) = \argmax_{k} p^*(Y=K|X) \quad \text{Bayes classifier rule}
      \end{equation*}
      \begin{equation*}
        \begin{align*}
        p(error_{Bayes}) &= \mathbb{E}_{x} [p(error(x))] \\
        &= \int (1 - \max_{k}(y = k|x))p^*(x)dx
        \quad x \in \mathbb{R}^{D}$ with $p^*(x)
        \end{align*}
      \end{equation*}

      \textbf{Definition: Decision regions are connected regions
       in $\mathbb{R}^D$ where $\hat{f}(x) = const$.}
    \subsection{Discriminative vs. Generative models}
      \\
      \begin{equation*}
        \text{Bayes Rule:} \quad p(Y|X) = \frac{p(X|Y)*p(Y)}{p(X)}
      \end{equation*}
      \begin{equation*}
        \text{posterior}: p(Y|X) \quad \text{likelihood}: p(X|Y) \quad \text{prior}: p(Y)
        \quad \text{data density/evidence}: p(X)
      \end{equation*}
      \textbf{discriminative model:} learn LHS of Bayes: p(Y|X) \\
      \textbf{generative model:} learn RHS of Bayes: p(Y), p(X|Y), p(X)
      \begin{equation*}
        p(X) = \sum_{K=1}^{N} p(X|Y=K)p(Y=K)
      \end{equation*}
      $\Rightarrow$ can be used to create more data by simulation, disadvantage:
      usually needs more training data for the same success rate
      \begin{equation*}
        C = \text{Bayes decision function} \quad f(x) = \begin{cases}
        +1 & \text{if} \quad p(y=1|x) \geq p(y = -1|x) \\
        -1 & else
        \end{cases}
      \end{equation*}
      \begin{equation*}
        \begin{align*}
          \frac{p(x|y=1)p(y=1)}{p(x)} &> \frac{p(x|y=-1)p(y=-1)}{p(x)} \\
          \iff \quad \frac{p(x|y=1) p(y=1)}{p(x|y=-1)p(y=-1)} &\geq 1 \\
          \iff \quad \log(p(x|y=1)p(y=1)) - \log(p(x|y=-1)p(y=-1)) &\geq 0 \\
          \iff \quad \log(\frac{p(x|y=1)}{p(x|y=-1)}) + log(\frac{\pi_{1}}{\pi_{-1}}) &\geq 0
        \end{align*}
      \end{equation*}
      \begin{equation*}
        \pi_{k} = \frac{1}{C} = const \quad \pi_{1} = \pi_{-1} = \frac{1}{C}
      \end{equation*}
      \begin{equation*}
        \Rightarrow \quad \text{max. likelihood decision rule} \quad
        f(x) = \begin{cases}
        1 & if \frac{p(x|y=1)}{p(x|y=-1)} \geq 1 \\
        -1 & else
        \end{cases}
      \end{equation*}
      \textbf{Example:} Y $\in$ {red, blue}, X $\in$ {ball pen, marker}
      \begin{equation*}
        \pi_{red} = \pi_{blue} = 0.5
      \end{equation*}
      \begin{equation*}
        p(marker|red) = \frac{5}{7} \quad p(marker|blue) = \frac{2}{7} \quad
        p(ball|red) = \frac{2}{7} \quad p(ball|blue) = \frac{5}{7}
      \end{equation*}
      \begin{equation*}
        \begin{align*}
          p(ball) &= p(ball|y = red)*p(red) + p(ball|y = blue)*p(blue) \\
          &= \frac{2}{7} * \frac{1}{2} + \frac{5}{7} * \frac{1}{2} = \frac{1}{2}
        \end{align*}
      \end{equation*}
      \begin{equation*}
        p(red|ball) = \frac{p(ball|red)}{p(ball)} = \frac{\frac{2}{7} * \frac{1}{2}}{\frac{1}{2}}
      \end{equation*}
      \begin{equation*}
        p(blue|ball) = \frac{5}{7} \quad p(red|marker) = \frac{5}{7} \quad p(blue|marker) = \frac{2}{7}
      \end{equation*}
      $\Rightarrow$ Bayes decision: \\
      \begin{equation*}
        f(x=marker) = \argmax_{k} p(y=k|marker) \quad \text{= red}
      \end{equation*}
      \begin{equation*}
        f(x=ball) = \argmax_{k} p(y=k)|ball)\quad \text{= blue}
      \end{equation*}
    \subsection{Nearest Neighbor Classification}
      Intuition: in an unknown situation, act as you did in the most similar situation in the past
      \begin{itemize}
        \item 'past': training set
        \item 'act as you did': copy the training label to the new instance
      \end{itemize}
      For 'most similar' we need  a distance function between features $d(x, x')$
      \begin{equation*}
        \text{decision rule}: f_{NN}(x) = y_{i}, \quad i = \argmin_{n \in Training set} d(x_{i}, x_{i'})
      \end{equation*}
      effect: split feature space according to the distanc to training examples
      \begin{equation*}
        neighbors(x_{i}) = \{ x|d(x, x_{i}) \leq d(x, x_{i'}) \} \quad \forall i \neq i'
      \end{equation*}
      'Voroni tessellation' with centers $\{ x_{i} \} _{i=1}^N$ \\
      each region is a voroni cell of $x_{i}$ \\
      decision boundaries: bisectors between centers
      \subsubsection{Performance Analysis of NN classifier}
        \begin{itemize}
          \item derive analytic formulas for error for finite training set, this ist the
          best, but usually very difficult
          \item derive analytic error formulas in the limit for infinitly many training data
          'asymptotic analysis', this is usually easier, but often unrealistic
          (when error decreases slowly with N)
          \item measure error empirically on independent test data('ground truth'):
          most ralistic, but must be repeated for every model and application, beware
          of the multiple testing bias if test data is reused
        \end{itemize}
        \paragraph{finite sample analysis}
          example:
          \begin{equation*}
            C=2 \quad y \in \{0, 1\}, \quad p(y=0) = p(y=1) = \frac{1}{2}
          \end{equation*}
          \begin{equation*}
            p(x|y=0) = 2 - 2x \quad p(x|y=1) = 2x
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              \int_{0}^{1} P(x|y=0)dx &= \int_{0}^{1} (2-2x)dx \\
              &= 2x - x^2  \Big | _{0}^{1} = 1
            \end{align*}
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              p(x) &= p(x|y=0)p(y=0) + p(x|y=1)p(y=1) \\
                   &= (2-2x)\frac{1}{2} + 2x \frac{1}{2} \\
                   &= 1 - x + x = 1
            \end{align*}
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              \text{postkrias} \quad p(y=0|x) &= \frac{p(x|y=0)p(y=0)}{p(x)} \\
              &= \frac{(2 - 2x)\frac{1}{2}}{1} \\
              &= 1 - x
            \end{align*}
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              p(y=1|x) = \frac{2x\frac{1}{2}}{1} = x
            \end{align*}
          \end{equation*}
          define two decision rules with 'threshold' t:
          \begin{equation*}
            A: \quad f_{A}(x;t) = \begin{cases}
            0 & \text{if $x\leq t$} \\
            1 & \text{if $x > t$}
            \end{cases}
          \end{equation*}
          \begin{equation*}
            B: \quad f_{B}(x;t) = \begin{cases}
            1 & \text{if $x\leq t$} \\
            0 & \text{if $x > t$}
            \end{cases}
          \end{equation*}
          \begin{equation*}
            \mathbbm{1}[condition] = \begin{cases}
              1 & \text{if condition = true} \\
              0 & \text{if condition = false}
            \end{cases}
            \quad \text{'Indicator function'}
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              p(A;t|error) &= \mathbb{E}_{x}[p(f_{A}(x;t) \neq Y^* |t)] \\
              &= \mathbb{E}_{x}[p(y=1|x) \mathbbm{1} [x\leq t]]
              + \mathbb{E}_{x}[p(y=0|x)\mathbbm{1} [x;t]] \\
              &= \int_{0}^{1}p(y=1|x)\mathbbm{1}[x\leqt]p(x)dx
              + \int_{0}^{1}p(y=0|x)\mathbbm{1}[x>t]p(x)dx \\
              &= \int_{0}^{1}p(y=1|x)p(x)dx + \int_{t}^{1}p(y=0|x)p(x)dx \\
              &= \int_{0}^{t}xdx + \int_{t}^{1} (1-x)dx \\
              &= \frac{x^2}{2} \Big |_{0}^{t} + (x-\frac{x^2}{2}) \Big |_{t}^{1} \\
              &= \frac{t^2}{2}-0+1-\frac{1}{2}-t+t^2 \\
              &= t^2 -t + \frac{1}{2} = (t-\frac{1}{2})^2 + \frac{1}{4} \\
              &= p(error|A,t)
            \end{align*}
          \end{equation*}
          \begin{equation*}
            p(error|B,t) = \frac{3}{4} - (t-\frac{1}{2})^2 = 1-p(error|A,t)
          \end{equation*}
          Bayes classifier(minimizes error): rate A with $t=\frac{1}{2}$
          \begin{equation*}
            p(error|A, t=\frac{1}{2}) = \frac{1}{4}
          \end{equation*}
          Error of NN classifier, simplest possible TS with N = 2:
          \begin{itemize}
            \item sample z = (x, y)
            \item repeat: sample z' = (x', y') until $y'\neq y$, 'rejection sampling'
          \end{itemize}
          Two possible outputs:
          \begin{equation*}
            \begin{rcases*}
              A: x_{0}(\corresponds y_{0} = 0) \leq x_{1} (\corresponds y_{1} = 1): rule A \\
              B: x_{0}(\corresponds y_{0} = 0) > x_{1} (\corresponds y_{1} = 1): rule B
            \end{rcases*}
            t = \frac{x_{0} + x_{1}}{2}
          \end{equation*}
          \begin{equation*}
            p(error) = \mathbb{E}_{TS}[p(error|TS)] = \mathbb{E}_{TS(A)}[p(error(A,t))] +
            \mathbb{E}_{TS(B)}[p(error|B, t)]
          \end{equation*}
          \begin{equation*}
            \begin{align}
            \mathbb{E}_{A} &= \int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \underbrace{p(error|A, t)}_{
            = (t-\frac{1}{2})^2+\frac{1}{4}}\underbrace{p(A, t|x_{0}, x_{1})}_{
            = \mathbbm{1}[x_{0} \leq x_{1}]\delta(t-\frac{x_{0}+x_{1}}{2})}
            \underbrace{p(x_{0}, x_{1})}_{=p(x|y=0)p(x|y=1)}dt dx_{1} dx_{0} \\
            &= \int_{0}^{1} p(x|y=0) \int_{x_{0}}^{1} p(x|y=1)p(error|A,t=\frac{x_{0}+x_{1}}{2})
            dx_{1} dx_{0} \\
            &= \int_{0}^{1} (2-2x) \int_{x_{0}}^{1} 2 x_{1}((\frac{x_{0}+x_{1}}{2} - \frac{1}{2})^2)
            + \frac{1}{4})dx_{1} dx_{0} \\
            &= \frac{83}{360}
          \end{align}
          \end{equation*}
          \begin{equation*}
            p(error|B) = \frac{43}{360} \Rightarrow p(error) = \frac{7}{20}
          \end{equation*}
        \paragraph{Cross Validation}
          We need: generalization error (on unseen, new data) p(error) \\
          We have: training error/fit error
          \begin{equation*}
            h_{TS} = \frac{1}{N} \sum_{i=1}^{N} \mathbbm{1}[f(x_{i}) \neq y_{i}']
          \end{equation*}
          \begin{equation*}
            p_{error} = h_{TS} + w_{model optimism} \quad w \geq 0
          \end{equation*}
          if $p_{error} - h_{TS} = w$ is big, the model overfits the training set.
          Many models tend to overfit quite badly. \\
          $\Rightarrow$ solutions: \begin{itemize}
            \item use more training data (expensive)
            \item use better models
            \item use regularization
          \end{itemize}
          e.q. nearest neighbor classifier $h_{TS}=0$ \\
          how to estimate the error?
          \begin{itemize}
            \item split the training set at random in two subsets for training
            and test
            \item train on the training subset
            \item calculate the error on the test subset
          \end{itemize}
          $\Rightarrow$ since the choice of training and testset was arbitrary,
          reverse their roles and repeat and take the average of the two error
          (2-fold cross validation) \\
          results are improved (error more reliable) by using more subsets
          'K-fold cross validation'
          \begin{itemize}
            \item bring data into a random order (random-shuffle)
            \item put the first $\frac{N}{K}$ instances into fold 1
            \item put the second $\frac{N}{k}$ instances into fold 2
            \item repeat for $l = 1,...,K$
            \item use all folds except fold l for training
            \item use fold l for testing
            \item compute means and variance of the K errors
            \item popular $K=2, 5, 10$ K = N: 'leave-one-out-cross-validations'
            for theoretical analysis
          \end{itemize}
        \paragraph{Asymptotic Analysis}
          \begin{itemize}
            \item find analytic formulas for how the method performs with
            infinite training data
            \item $N \rightarrow \infty$ (training data)
            \item Definition: A learning algorithm is called consistent if
            it converges to the optimal Bayes classifier as
            $N \rightarrow \infty$
            \item prove now: NN classifier is \underline{not} consistent, but
            not too far of (a factor of 2): $p_{00}^{NN} \leq 2p*$
            \item let $p(error|x, x')$ be the expected error for test point
            x, when x' is its nearest training point
            \item let p(x|x') be the probabilty that x' is n.n. of test point
          \end{itemize}
          \begin{equation*}
            \begin{align}
              p(error|y) &= \int p(error|x, x')p(x'|x)dx' \quad \text{(marginalize over
              unknown point x')} \\
              &= \mathbb{E}_{x'}[p(error|x, x')]
            \end{align}
          \end{equation*}
        \paragraph{1)}
          If density p(x) is continous and positive:
          \begin{equation*}
            \lim_{N \to \infty} p(x'|x) = \delta(x-x')
          \end{equation*}
          Let $p_{\varepsilon}(x)$ be the probability that an $\varepsilon$-ball around x:
          \begin{equation*}
            B_{\varepsilon}(x) = \{ x'| \lVert x-x' \rVert \leq \varepsilon \}
          \end{equation*}
          contains at least one training point. Then $(1-p_{\varepsilon})^N$
          is the probabilty, that none of N training points is in $B_{\varepsilon}(x)$
          \begin{equation*}
            \text{By assumption} \quad \forall \varepsilon > 0 \quad p_{\varepsilon}(x)
            = \int_{B_{\varepsilon}(x)} p(x') dx' > 0
          \end{equation*}
          \begin{equation*}
            \lim_{N \to \infty} (1-p_{\varepsilon}(x))^N = 0 \Rightarrow
            \forall \varepsilon > 0 \quad \text{there is a point in $B_{\varepsilon}(x)$}
          \end{equation*}
        \paragraph{2)}
          \begin{equation*}
            \begin{align}
              p(error|x,x') &= 1-p(correct|x,x') \\
              &= 1-\sum_{k=1}^{c}p(y=k,y'=k|x,x') \\
              &= 1-\sum_{k=1}^{c}p(y=k|x)p(y'=k|x') \quad \text{due to i.i.d.}
            \end{align}
          \end{equation*}
        \paragraph{3)}
          \begin{equation*}
            \begin{align*}
              \text{Insert:} \quad p_{\infty}(error|x) &= \int \underbrace{
              p(error|x,x')}_{1-\sum_{i}^{c}}\underbrace{p(x'|x)}_{\delta(x-x')}dx' \\
              &= 1- \sum_{k=1}^{c}p(y=k|x)^2 \quad \text{Gini impurity at point x}
            \end{align*}
          \end{equation*}
          \begin{itemize}
            \item if data at point x are pure, i.e. only one class occurs,
            say $y=k^* \Rightarrow p(y=k^*|x)=1$ and $p(y=k|x)=0$ for $k\neq k^*
            \Rightarrow p_{\infty}(error|x) = 0$
            \item worst: data are impure, i.e. all classes gave same probability
            $p(y=k|x) = \frac{1}{c} \Rightarrow$
            \begin{equation*}
              p_{\infty}(error|x)=1-\sum_{k}^{c} \frac{1}{c^2} = 1-\frac{1}{c}
              = \frac{c-1}{c} \geq \frac{1}{2}
            \end{equation*}
          \end{itemize}
        \paragraph{4)}
          Derive worst case behavior aver all x as a function of Bayes error
          $p^*$
          \begin{equation*}
            p_{\infty}(error) = \mathbb{E}_{x}[p_{\infty}(error|x)]
          \end{equation*}
          Let $p(y=\hat{k}|x)$ be the Bayes decision at x, $\hat{k} =
          \argmax_{k}p(y=k|x)$ \\
          $\Rightarrow$ Bayes error at x:
          \begin{equation*}
            p^*(error|x) = 1-p(y=\hat{k}|x)
          \end{equation*}
          \begin{equation*}
            \sum_{k=1}^{c} p(y=k|x)^2 = (1-p^*(error|x))^2 + \sum_{k=\hat{k}}
            p(y=k|x)^2
          \end{equation*}
          worst case analysis: make the error big, i.e. make this sum small
          \begin{equation*}
            \text{Probability:} \quad \sum_{k} p_{k}^2 \quad \text{is minimized
            under constrains} \quad p_{k} \geq 0 \quad \text{and}\quad \sum_{k}p_{k}=const
          \end{equation*}
          \begin{equation*}
            if \quad p_{k}=p_{k}' \forall k, k' \quad p_{k} = p^*(error|x)
          \end{equation*}
          \begin{equation*}
            \Rightarrow p_k = \frac{p^*(error|x)}{c}
          \end{equation*}
          worst case error:
          \begin{equation*}
            \begin{align*}
            \sum_{k=1}^{c} p(y=k|x) &\geq (1-p^*(error|x))^2 + \sum_{k=k'}
            (\frac{p^*(error|x)}{c- \frac{1}{2}})^2 \\
            &=1-2p^*(error|x)+p^*(error|x)+\frac{p^*(error|x)^2}{c-1}
            &=1-2p^*(error|x) + \frac{c}{c-1}p^*(error|x)^2
            \end{align*}
          \end{equation*}
        \paragraph{5)}
          Inserting gives the relationship between error of NN classifier and
          Bayes classifier:
          \begin{equation*}
            \begin{align*}
              p_{\infty}(error|x)=1-\sum_{k}^{c}p(y=k|x) &\leq 1-(1-2p^*(error
              |x))+\frac{c}{c-1}p^*(error|x)^2 \\
              &=2p^*(error|x)-\frac{c}{c-1}p^*(error|x)^2
            \end{align*}
          \end{equation*}
        \paragraph{6)}
          Total error = expectation over x
          \begin{equation*}
            \begin{align*}
              p_\infty(error)&=\mathbb{E}_x[p_\infty(error|x)] = \int p_\infty
              (error|x)p(x)dx \\
              &\leq \int 2p^*(error|x)p(x)dx-\int \frac{c}{c-1}p^*(error|x)^2
              p(x)dx \\
              &=2\mathbb{E}_x[p^*(error|x)]-p^*(error)  \\
              &\leq2p^*(error)-\frac{c}{c-1}p^*(error)^2
            \end{align*}
          \end{equation*}
          simplified by non neg. of variance:
          \begin{equation*}
            \begin{multlined}
              \int(p^*(error|x)-p^*(error))^2 p(x)dx \geq 0 \\
              \leftrightarrow \int p^*(error|x)^2p(x)dx \geq p^*(error)^2
            \end{multlined}
          \end{equation*}
          \begin{equation*}
            \text{Result:} \quad p^*(error) \leq p_\infty^NN (error) \leq p^*(error)(2-\frac{c}{c-1}
            p^*(error))
          \end{equation*}
          Special Cases:
          \begin{itemize}
            \item best case: $p^*=0 \Rightarrow p^\infty \leq 0 \quad 0(2-\frac{c}
            {c-1}0)=0 \quad \text{NN is perfect}$
            \item worst case: $p^*=\frac{c-1}{c} \quad \text{(pure guessing)}
            \Rightarrow$
            \begin{equation*}
              \begin{align*}
                p_\infty &< \frac{c-1}{c}(2-\frac{c}{c-1}\frac{c-1}{c}) \\
                &= \frac{c-1}{c}
              \end{align*}
            \end{equation*}
            \item normal case: Bayes classifier performs well, but not perfect:
            \begin{equation*}
              \begin{multlined}
                p^* = \varepsilon \ll 1 \forall c \geq 2: \frac{c}{c-1} \leq 2 \\
                p_\infty \leq \varepsilon(2-\underbracket{\frac{c}{c-1}\varepsilon}_{\ll 1})
                \leq 2\varepsilon = 2p^*
              \end{multlined}
            \end{equation*}
            Advantages of NN-method:
            \begin{itemize}
              \item simple and intuitive
              \item often easy to implement
              \item performs elecently in practice
            \end{itemize}
          \end{itemize}
      \subsubsection{Limitations of Nearest Neighbor Classifier}
        \paragraph{1.}
          NN is not consistent: $p_\infty(error) \leq 2p^*(error)$ (consistent:
          $p_\infty(error) = p^*(error)$) \\
          solution: K-nearest neighbor algorithm:
          \begin{itemize}
            \item find the k nearest neighbors
            \item take majority vote
            \item is consistent if $k(N)$ such that
            \begin{equation*}
              \lim_{N \to \infty} k(N) = \infty, \quad \lim_{N \to \infty}
              \frac{k(N)}{N}=0
            \end{equation*}
            \item e.g. k(N) $\log N$
          \end{itemize}
        \paragraph{2.}
          nearest neighbor search is expensive: naive algorithm $\mathcal{O}
          (D*N)$ D: #feature dimension, N: #instances \\
          solutions:
          \begin{itemize}
            \item reduce D:
            \begin{itemize}
              \item dimension reduction(later, ch.'unsupervised learning')
              \item relevant feature selection
            \end{itemize}
            \item reduce N, relevant instance selection e.g.:
            \begin{itemize}
              \item sort TS randomly
              \item memorize the next instance only if the memorized
              set so far classifies incorrect
            \end{itemize}
          \end{itemize}
          exactly:
          \begin{itemize}
            \item compute Voronoi tesselation
            \item drop all instances whose neighbors all have the same class
            \item clustering: find groups of similar instances ('clusters')
            which can be replaced by simple representative (later in chapter
            'unsupervised learning')
          \end{itemize}
          use an efficient search algorithm: D-dimensional search trees('k-d
          tree, x tree, $R^H$ tree') \\
          use approximate n.n. search: find a near neighbor fast and with high
          probability of being correct $\Rightarrow$ several ANN libraries in
          the internet
        \paragraph{3.}
          nearest neighbor selection depends on the distance function $d(x,x')$.
          How to define a 'good' $d()$? \\
          Depending on units, different neighbor might minimize $d(x,x')$
          solution. Therefor use dimensionless features, i.e. standardize data.
          Divide each feature by its TS standard deviation [actually, also
          substract means - 'centralization'] \\
          Better solution: learn the metric from the TS (or from additional
          TS with 'is similar'/'is not similar')-labels. \\
          $\Rightarrow$ research area: 'metric learning' \\
          Much of the success of neural networks is their ability to
          implicilly find a good set of intermediate features and metric.
    \subsection{Quadratic and Linear Discriminant Analysis}
      \subsubsection{Motivation}
        In nearest neighbors, we can reduce search time by reducing N
        $\Rightarrow$ extreme case: One representatice per label. \\
        Obvious choice is the mean of each class:
        \begin{equation*}
          \forall k \in 1,...,c: \quad \mu_k = \frac{1}{N_k} \sum_{i:y_i=k}x_i
          \quad N_k = \text{#instances in class k}
        \end{equation*}
        This works well, if clusters are roughly circular. \\
        To find the desired decision bound, we neet to consider the
        actual shape of the class $\Rightarrow$ correction for non-circularity \\
        Simplest generalization: approximate cluster shape by an ellipse instead
        of circle (higher dimension: ellipsoid) \\
        $\Rightarrow$ Natural choice: multi-dimensional Gaussian distribution
      \subsubsection{QDA}
        assumptions:
        \begin{itemize}
          \item each class prior $p(y=k)$ is well approximatet by the TS
          proportion $\hat{p}(y=k)=\frac{N_k}{N}=\pi_k$
          \item the data likelihood for each class p(x|y=k) is well approximated
          by a Gaussian distribution
        \end{itemize}
        $\Rightarrow$ generative model:
        \begin{equation*}
           p(y=k|x) = \frac{p(x|y=k)p(y=k)}{p(x)}
        \end{equation*}
        \begin{equation*}
          p(x) = \sum_{k=1}^{c}p(x|y=k)p(y=k)
        \end{equation*}
        \paragraph{Gaussian distribution}
          \begin{equation*}
            p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{x-\mu}{2\sigma^2}}
          \end{equation*}
          generalization of $\sigma^2$: $\Sigma$ covariance matrix D x D,
          symmetric, positive definite
          \begin{equation*}
            \Sigma = \frac{1}{N}\sum_i(x_i-\mu)(x_i-\mu)^T
          \end{equation*}
          \begin{equation*} \text{for example:} \quad
            \Sigma = R^{-1}\begin{pmatrix}
            \sigma_1^2 & 0 \\
            0 & \sigma_2^2
            \end{pmatrix} R
          \end{equation*}
          principle axes of ellipse: directin of largest and smallest width
          $\leftrightarrow$ eigenvectors of covariance matrix $\Sigma$
          \begin{equation*}
            \text{multivariable Gaussian} \quad p(x;\vec{\mu},\Sigma) =
            \frac{1}{\sqrt{det(2\pi\Sigma)}}e^{-\frac{1}{2}(x-\vec{\mu})^T
            \Sigma^{-1}(x-\vec{\mu})}
          \end{equation*}
          To find $p(x|y=k)$, we must define $\underbrace{\vec{\mu_{k}}}_{\text
          {cluster pos.}}, \underbrace{\Sigma_{k}}_{cluster shape}$ for
          each k:
        \paragraph{How to fit a multi-dim. Gaussian}
          Let $\{ x_{i}\}_{i=1}^N$ be the instance of just a single class (drop
          index k for clarity).
          \begin{equation*}
            p(x) = \text{multi-dim. Gaussian}
          \end{equation*}
          Fit according to maximum likelihood principle assumed that TS is
          typical for true (unknown) distribution. \\
          $\Rightarrow$ we want the TS to be typical for our model as well,
          when simulating our model, the TS should occur with high (maximum)
          probability.
          \begin{equation*}
            \begin{align*}
              p(TS) &\hspace{1ex}= p(x_1,...,x_N) \Rightarrow \text{maximized under our model} \\
              &\stackrel{i.i.d.}{=} p(x_1;\mu,\Sigma)p(x_2;\mu,\Sigma)...p(x_N,\mu,\Sigma)
            \end{align*}
          \end{equation*}
          \begin{equation*}
            \log(p(x_1,...,x_N)) = \sum_{i=1}^{N}\log(p(x_i;\mu,\Sigma))
          \end{equation*}
          To maximize, take derivatives with respect to parameter set to 0 \\
          $\Rightarrow$ system of equations, solve to find
          $\hat{\mu}, \hat{\Sigma}$
          \begin{equation*}
            \frac{d\sum_i\log(x_i;\mu,\Sigma)}{d\mu} \stackrel{!}{=} 0 \quad
            \frac{d\sum_i\log(x_i;\mu,\Sigma)}{d\Sigma} \stackrel{!}{=} 0
          \end{equation*}
        \paragraph{How to fit a Gaussian?}
          maximize likelihood of TS:
          \begin{equation*}
            \begin{align*}
              \hat{\Theta} &\hspace{1ex}= \argmax \log p(x_1,...x_n|\Theta) \\
              &\stackrel{i.i.d.}{=} \argmax \sum_{i=1}^{N} \log p(x_1,...x_n|\Theta)
            \end{align*}
          \end{equation*}
          \begin{equation*}
            p(x_i;\Theta) = \frac{1}{\sqrt{\text{det}(2\pi\Sigma)}}e^{-
            \frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)}
          \end{equation*}
          \textbf{Define: } $K=\Sigma^{-1} \quad$ 'precession matrix'
          \begin{equation*}
            \text{Lin. Algebra:} \quad \text{det}(a*A) = a^D\text{det}A \quad \text{det}
            (A^{-1})=\frac{1}{\text{det}(A)} \quad \text{a: Scalar, A: D*D Matrix}
          \end{equation*}
          \begin{equation*}
            \Rightarrow \frac{1}{\sqrt{\text{det}(2\pi\Sigma)}}=
            (2\pi)^{-\frac{D}{2}}\frac{1}{\sqrt{\text{det}(K^{-1})}}=
            (2\pi)^{-\frac{D}{2}}\text{det}(K)^{\frac{1}{2}}}
          \end{equation*}
          \begin{equation*}
            \log p(x_i;\mu,K)=\sum_{i=1}^{N}-\frac{D}{2}\log2\pi+\frac{1}{2}
            \log\text{det}(K)-\frac{1}{2}(x_i-\mu)^TK(x_i-\mu)
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              \text{find $\mu$ by} \quad &\frac{\partial}{\partial \mu}
              \log p(x_i,...x_n) \overset{!}{=} 0 \\
              \iff &\sum_{i=1}^{N}-K(x_i-\mu) = 0 \\
              \iff &\sum_{i=1}^{N}(x_i-\mu)=0 \\
              \iff &\sum_{i=1}^{N}x_i = \sum_{i=1}^{N}\mu = N\mu \\
              \implies &\mu =\frac{1}{N}\sum_{i=1}^{N}x_i \quad
              \text{empirical mean of TS}
            \end{align*}
          \end{equation*}
          \begin{equation*}
            \text{Lin. Algebra:} \quad \frac{\partial}{\partial v}v^TAv=2Av
          \end{equation*}
          \begin{equation*}
            \begin{align*}
              \text{find K:} \quad &\frac{\partial}{\partial K}
              \log p(x_1,...,x_N) \overset{!}{=}0 \\
              \iff &\sum_{i=1}^{N}(\frac{1}{2}K^{-1}-\frac{1}{2}z_iz_i^T)=0 \quad
              \text{centered coordinate } z_i=x_i-\mu
            \end{align*}
          \end{equation*}
          \begin{equation*}
            \text{Matrix calculus:} \quad \frac{\partial}{\partial K} \log\text{det}(K^T)^{-1}=(K^T)^{-1}=K^{-1}=\Sigma
          \end{equation*}
          \begin{equation*}
            \frac{\partial}{\partial A} v^TAv=vv^T
          \end{equation*}
          \begin{equation*}
            N\Sigma=\sum_{i=1}^{N}\Sigma=\sum_{i=1}^{N}\underbrace{(x_i-\mu)(x_i-\mu)^T}_{scatter matrix}
          \end{equation*}
          \begin{equation*}
            \Sigma=\frac{1}{N}\sum_{i=1}^{N}(x_i-\hat{\mu})(x_i-\hat{\mu})^T
            \quad \text{sample/empirical covariance matrix}
          \end{equation*}

        \paragraph{Training of QDA} \\
          Repeat this for every class to get $\mu_1,\Sigma_1,...,\mu_c,\Sigma_c$ \\
          QDA prediction: -generative model:
          \begin{equation*}
            \begin{align*}
              \hat{k} &= \argmax_k p(y=k]x) \\
              &= \argmax_k \frac{p(x|y=k)p(y=k)}{p(x)} \\
              &= \argmax_k \underbrace{p(x|y=k)}_{Gauss(x,\mu_k,\Sigma_k)}\underbrace{p(y=k)}_{\pi_k= \frac{N_k}{N}} \\
              &= \argmin_k -\log p(y=k|x) \\
              &= \argmin_k \frac{D}{2} \log 2\pi+\frac{1}{2} \log \text{det} \Sigma_k+\frac{1}{2}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)-\log \pi_k \\
              &= \argmin_k \cancel{\frac{1}{2}}\underbrace{\log \text{det} \Sigma_k -2 \log \pi_k}_{=b_k} + \cancel{\frac{1}{2}} (x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k) \\
              &= \argmin_k \underbrace{(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)}_{\text{squared Mambalanbis distance btw. }x \text{ and } \mu_k} + b_k
            \end{align*}
          \end{equation*}
          Euclidean: $\Sigma = \mathbbm{1} \quad \sigma \neq \mathbbm{1} \quad$ adjust for elliptic cluster sphere. \\ \\
          Define square root of matrix $A^{\frac{1}{2}} \iff A=(A^{\frac{1}{2}})^T(A^{\frac{1}{2}})$ \\ \\
          Find $\Sigma^{-1}$ by decomposition $z_k=\Sigma_k^{-\frac{1}{2}}(x-\mu_k)$ \\
          \begin{equation*}
            \Rightarrow z_k^Tz_k=(x-\mu_k)^T\frac{\Sigma_k^{-\frac{1}{2}T}\Sigma_k^{-\frac{1}{2}}}{\Sigma_k^{-\frac{1}{2}}}(x-\mu_k)
          \end{equation*}
          $\Rightarrow$ can use standard nordmed of $z_k$


      \subsubsection{LDA Linear Discriminant Analysis}
        simplifications: assume that clusters have the same size elliptic shape
        \begin{equation*}
          \forall k, k' \quad \Sigma_k = \Sigma_{k'} = \Sigma
        \end{equation*}
        \begin{equation*}
          QDA: \quad \hat{k} = \argmin_{k}(x-\mu_k)^T\Sigma_k^{-1}(x-\mu_k)+b_k
        \end{equation*}
        \begin{equation*}
          \begin{align*}
            LDA: \quad \hat{k} &=\argmin_k(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \\
            &=\argmin_k x^T\Sigma^{-1}x-\Sigma\mu_k^T\Sigma^{-1}x+\mu_k^T\Sigma^{-1}\mu_k \\
            &=\argmin_k -\Sigma \mu_k^T\Sigma^{-1}x+\mu_k^T\Sigma^{-1}\mu_k+b_k \\
            &=\argmin_k w_k^Tx+b_k'
          \end{align*}
        \end{equation*}
        \textbf{New variables:} $\quad w_k^T=\Sigma\mu_k^T\Sigma^{-1}
        \quad b_k'=-\mu_k^T\Sigma^{-1}\mu_k-b_k$ \\
        LDA is a particular way to define $w_k$ and $b_k'$. There are many other
        possible ways, e.g. logistic regression. \\ \\
        \underline{Special case}: C=2
        \begin{equation*}
          \begin{align*}
          \hat{k} &=
            \begin{cases}
              0 & \text{if} \quad w_0^Tx+b_0' > w_1^Tx+b_1' \\
              1 & \text{if} \quad w_0^Tx+b_0' < w_1^Tx+b_1'
            \end{cases} \\
            &=\begin{cases}
              0 & \text{if} \quad (w_0^T-w_1^T)x+b_0'-b_1' > 0 \\
              1 & \text{otherwise}
            \end{cases} \\
            &=\begin{cases}
              0 & \text{if} \quad w^Tx+b' > 0 \\
              1 & \text{if} \quad w^Tx + b' < 0
            \end{cases}
          \end{align*}
        \end{equation*}
        \textbf{Define:} $w=w_1-w_0 \quad b'=b_1'-b_0'$ \\
        $a=w^Tx$ is a 1D projection of x onto the vector w \\
        $\Rightarrow$ apply the mean classifier to the a value
      \subsubsection{LDA}
      Two classes C = 2
      \begin{equation*}
        \hat{y} = \begin{cases}
        1 & \text{if } w^Tx + b \geq 0 \\
        0 & else
        \end{cases}
      \end{equation*}
      \underline{Three different derivations of how to choose the best w and b}: \\
      \paragraph{1.}
        our derivation: LDA is the same as QDA with all classes having the same
        cluster shape $\Rightarrow$ we just fit a Gaussian distribution to within - class
        covariance
        \begin{equation*}
          \hat{\Sigma} = \frac{1}{N} \Sigma_{i=1}^{N} (x_{i} - \mu_{k = y_{i}}) ( x_{i} - \mu_{y_{i}})^T
        \end{equation*}
        \begin{equation*}
          w^T = 2(\mu_{1} - \mu_{0}) \hat{\Sigma}^{-1}
        \end{equation*}
      \paragraph{2.}
        R. Fishers original derivation: we seek the optimal 1-dimensional
        projection of D-dimensional data
        \begin{itemize}
          \item compute $\mu_{0}, \mu_{1}, \Sigma$
          \item define 1-D projection $z_{1} = w^Tx_{i} \Rightarrow$ projected
          means $m_{k} = w^T\mu_{k}$, 1-D variance $\sigma^2 = w^T\Sigma$
          \item determine w such that the $z_{i}$ are seperated as good as possible
          \begin{equation*}
            m_{0} - m_{1} = w^T(\mu_{1} - \mu_{0})
          \end{equation*}
          \item Fisher criterion: Choose w that maximizes $\frac{(m_{1} - m_{0})^2}{\sigma^2}$
        \end{itemize}
      \paragraph{3.}
        derivation via least-squares regression \\
        define class labels $Y \in \{ -1, 1\}$ \\
        LSQ: find w, b that minimzes:
        \begin{equation*}
          \sum_{i=1}^{N}(w^Tx_{i} + b_{i} - y_{i})^2
        \end{equation*}
        if classes are balanced: $N_{-1} = N_{+1}$ \\
        $\Rightarrow$ optimal solution is again the same $\Rightarrow$ proof: home-work
    \subsection{Logistic Regression LR}
      \begin{itemize}
        \item Not really regression, because it's a classifier, term is partially justified because LR
          predicts class probabilities. It actually computs the posterior $p(Y|X)$
        \item generative model (LDA) vs. discriminative model (LR)
        \item LDA:
        \begin{itemize}
          \item define RHS of Bayes theorem (likelihood and prior)
          \item learn the parameters of RHS (fit a Gaussian for every class)
          \item  apply Bayes to compute the posterior
        \end{itemize}
        \item LR
        \begin{itemize}
          \item define RHS of Bayes
          \item apply Bayes to compute posterior (LHS)
          \item learn parameters of the LHS
          \item or: merge first two steps and define LHS model directly (e.g. NN)
        \end{itemize}
      \end{itemize}
      derive LR from LDA: 2 classes C=2, equal priors $p(y=0) = p(y=1) = \frac{1}{2}$,
      cluster shape (e.g. covariance $\Sigma$ of both classes equal)
      \begin{equation*}
        \begin{align*}
          p(y=1|x) &= \frac{p(x|y=1)\cancel{p(y=1)}}{p(x|y=0)\cancel{p(y=0)} + p(y|y=1)\cancel{p(y=1)}} \\
          &= \frac{p(x|y=1)}{p(x]y=0) + p(x|y=1)} \\
          &= \frac{p(x|y=1)}{p(x|y=1)} * \frac{1}{\frac{p(x|y=0)}{p(x|y=1)} + 1}
        \end{align*}
      \end{equation*}
      Gaussian likelihoods:
      \begin{equation*}
        p(x|y=0) = \frac{1}{\sqrt{det(2\pi\Sigma)}} e^{-\frac{1}{2}(x-\mu_{0})^T\Sigma^{-1}(x - \mu_{0})}
      \end{equation*}
      \begin{equation*}
        p(x|y=1) = \frac{1}{\sqrt{det(2\pi\Sigma)}} e^{-\frac{1}{2}(x-\mu_{1})^T\Sigma^{-1}(x - \mu_{1})}
      \end{equation*}
      \begin{equation*}
        \mu = p(y=0)\mu_{0} + p(y=1)\mu_{1} = \frac{\mu_{0} + \mu_{1}}{2} = 0
      \end{equation*}
      assume that dara are centered $\Rightarrow \mu = 0$
      \begin{equation*}
        \begin{align*}
        p(y=1|x) &= \frac{1}{1 + \frac{p(x|y=0)}{p(x|y=1)}} \\
        &= \frac{1}{1 +\exp(-0.5[(x_\mu_{1})^T\Sigma^{-1}(x+
        \mu_{1})- (x-\mu_{1})^T\Sigma^{-1}(x-\mu_{1})])} \\
        &= \frac{1}{1 + exp(-0.5[4\mu_{1}^T\Sigma^{-1}x])} \\
        &= \frac{1}{1 + exp(-2\mu_{1}^T\Sigma^{-1}x)} \\
        &= \frac{1}{1 + exp(-w^Tx)}
        \end{align*}
      \end{equation*}
      \begin{equation*}
        w^T = 2\mu_{1}^T\Sigma^{-1}
      \end{equation*}
      \begin{equation*}
        p(y=0|x) = 1 - p(y=1|x)
      \end{equation*}
      \begin{equation*}
        \underline{Decision rule}: \quad \hat{y} \begin{cases}
        1 & if \quad p(y=1|x) \geq p(y=0|x) = 1-p(y=1|x) \\
        0 & otherwise
        \end{cases}
      \end{equation*}
      \begin{equation*}
        \text{logistic sigmoid function :} \quad \sigma(t) = \frac{1}{1 + exp(-t)}
      \end{equation*}
      Properties:
      \begin{equation*}
        \sigma(-t) = 1 - \sigma(t)
      \end{equation*}
      \begin{equation*}
        \sigma(-t) = \frac{1}{1 + exp(t)} = 1- \frac{1}{1+ exp(-t)} = \frac{exp(-t)}{1+ exp(-t)} = \frac{1}{exp(t) + 1}
      \end{equation*}
      Derivative:
      \begin{equation*}
        \frac{d}{dt} \sigma(t) = \frac{d}{dt} (1+exp(-t))^{-1} = (1+exp(-t))^{-2}exp(-t) = \sigma(t)\sigma(-t) = \sigma(t)(1 - \sigma(t))
      \end{equation*}
      \subsubsection{Learning LR}
        \begin{itemize}
          \item maximum likelihood principle: choose the model such that the training data are a typical
          realization means:
          \begin{equation*}
            \begin{align*}
              \hat{w} &= \argmax_{w} p((x_{1}, y_{1}), ..., (x_{n}, y_{n})|w) \\
              &= \argmin_{w} - \log p((x_{i}, y_{i})|w) \\
              &\stackrel{i.i.d.}{=} \argmin_{w} -\Sigma_{i=1}^{N} \log p(y=y_{i}|x_i,w) \\
              &= \argmin_{w} -[\Sigma_{i, y_{i} = 1} \log p(y=1|x_{i},w) +
               \Sigma_{i, y_{i} = 0} \log(1-p(y=1|x_{i},w))] \\
              &= \argmin_{w}  = -\Sigma_{i=1}^{N} [y_{i} \log p(y=1|x_{i},w) +
              (1- y_{i})\log(1-p(y=1|x_{i},w))]
            \end{align*}
          \end{equation*}
        \end{itemize}
        \paragraph{Find w}

        \begin{equation*}
          \begin{align*}
          \frac{\partial}{\partial w} -\sum_{i=1}^{N}... &= -\sum_{i=1}^{N}[y_i \frac{1}{\sigma(w^Tx_i)}\sigma(w^tx_i)\sigma(-w^Tx_i)x_i +
          (1-y_i) \frac{1}{1-\sigma(w^Tx_i)}(-1)\sigma(w^Tx_i)\sigma(-w^Tx_i)x_i] \\
          &= -\sum_{i=1}^{N} (y_{i} \underbrace{\sigma(-w^T x_i)}_{1-\sigma(w^Tx_i)}x_i - (1- y_i)\sigma(w^T x_i)x_i) \\
          &= - \sum_{i=1}^{N} y_i x_i - y_i\sigma(w^T x_i) x_i - \sigma(w^Tx_i)x_i + y_i \sigma(w^T x_i) x_i \\
          &= \sum_{i=1} (y_i - \sigma(w^T x_i)) x_i \overset{!}{=} 0
          \end{align*}
        \end{equation*}

        no analytical solution $\Rightarrow$ need to solve numerically \\
        Numerical algorithms:
        \begin{itemize}
          \item classical: few training data $N \leq 1000 \Rightarrow$ use Newton-Raphson algorithm $\Rightarrow$ Iterative Reweighted Least Squares (RLS $\Rightarrow$ later)
          \begin{itemize}
            \item advantage: needs few iterations
            \item drawback: each iteration is expensive when N gets bigger $\mathcal{O}(N^3)$ or $\mathcal{O}(N^2)$ with tricks
          \end{itemize}
          \item modern: lots of training data: stochastic gradient descent
          \begin{itemize}
            \item choose initial guess for $w^{(0)}$ e.g. $w^{(0)} = 0 \Rightarrow \sigma(w^T x) = \frac{1}{2} \quad \forall x \Rightarrow p(y=1|x) = p(y=0|x)$
            \item for t = 1, ..., T (or until convergence)
            \begin{itemize}
              \item bring TS into random order (e.g. random shuffle) of indices
              \item for i = 1,..., N: $w' = w - \tau(y_{i} - \sigma(w^T x_{i})x_{i}), \quad \tau: \text{learning rate}$
              \item reduce learning rate $ \tau \leftarrow \frac{t}{t-1} \tau$
            \end{itemize}
          \end{itemize}
        \end{itemize}
    \subsection{Histogramms and Density Trees}
      \subsubsection{Introduction}
        \begin{itemize}
          \item we had: generative models vs. discriminative models (learn LHS or RHS of Bayes)
          \item new distinction:
          \begin{itemize}
            \item parametric models: choose probabilities from a family with analytic formula (Gaussian)
            and we learn its parameters (Gaussian: $\mu,\Sigma$)
            \item non-parametric models: don't restrict the probability - 'universal model'
            (neural net) and learn many more parameters (network weights)
          \end{itemize}
        \end{itemize}

        \begin{center}
          \begin{tabular}{ |c|c|c|c| }
            \hline
              & generative & discriminative  \\
            \hline
            \multirow {}  parametric & LDA and QDA & LR \\
            non-parametric & histogramm, density tree & nearest neighbors \\
            \hline
          \end{tabular}
        \end{center}
        \begin{itemize}
          \item histogramm: count the frequency of random events: \\
          \begin{equation*}
            \frac{\sum_{i=1}\mathbbm{1}[x_i=z]}{N}
          \end{equation*}
            z: events considered and create table for all events
           \item x is discrete, throwing dice:  $z \in \{ 1,...,6\}$
           \begin{equation*}
              \text{hist}(z)=\frac{#[x=z]}{N}
           \end{equation*}
           \item x is continous $x \in R \Rightarrow$ discretize into \underline{bins}
           \begin{equation*}
             b_l = \{x|\underbrace{x_{min}+l\Delta x}_{x_l} \leq x \leq \underbrace{x_{min}+(l+1)\Delta x}_{x_{l+1}}\}
           \end{equation*}
           \item find bin index of x:
           \begin{equation*}
             l = \floor*{\frac{x-x_{min}}{\Delta x}} \quad \Delta x: \text{ bin width}\quad  \floor*{}:\text{ floor function}
           \end{equation*}
           \begin{equation*}
             \mathbbm{1}[a] = \begin{cases}
             1 & \text{if a is true} \\
             0 & \text{else}
             \end{cases}
           \end{equation*}
           $p_l$= prob for X in bin l
           \item approx likelihood:
           \begin{equation*}
             p(x|y) \approx \sum_l p(l) \mathbbm{1} \floor*{x \in b_l}
           \end{equation*}
           \item meaning: piecewise constant approximation
           $\Rightarrow$ can make error arbitrarily small by choosing more bins, but not
           more than the TS allows
           \item optimal approx, given TS and $\Delta$x
           \end{itemize}
           \begin{equation*}
             \hat{p}=\argmin_p Error = \int\underbrace{(p^*(x)-p(x))^2dx}_{\cancel{p^*(x)^2}-2p^*(x)p(x)+p(x)^2} \quad p^*: \text{ truth} \quad \hat{p}: \text{ best approx.}
           \end{equation*}
           \begin{equation*}
             \begin{align*}
               -2\intp^*(x)p(x)dx &= -2 \int p^*(x)\sum_l p(l) \mathbbm{1}[x\in b_l]dx \\
               &=-2\sum_l p(l) \int p^*(x) \mathbbm{1}[x \in b_l] dx \\
               &=-2 \sum_l p(l) \int_{x_l}^{x_{l+1}} p^*(x)dx \\
               &=-2 \sum_l p(l) \mathbb{E}_{p^*(x)}[\mathbbm{1}[x \in b_l]]
             \end{align*}
           \end{equation*}
           \begin{equation*}
             \begin{align*}
               \mathbb{E}_{p^*(x)}[\mathbbm{1}(x \in b_l)] \approx \frac{N_l}{N} \quad \N_l=#(x \text{ is in }l)
             \end{align*}
           \end{equation*}
           \begin{equation*}
             \mathbbm{1}[x \in b_l]^2 = \mathbbm{1}[x \in b_l] \quad l\neq l': \quad \mathbbm{1}[x \in b_l]\mathbbm{1}[x \in b_{l'}]=0
           \end{equation*}
           \begin{equation*}
             \begin{align*}
               \int p(x)^2dx &= \int(\sum_l p_l \mathbb{1}[x \in b_l])^2 \\
               &=\int \sum_l p_l^2 \mathbbm{1}[x \in b_l]dx \\
               &= \sum_l p_l^2 \int \mathbbm{1}[x \in b_l] dx \\
               &= \sum_l p_l^2 \int_{x_l}^{x_{l+1}}1dx \\
               &= \sum_l p_l^2 \Delta x
             \end{align*}
           \end{equation*}
           \begin{equation*}
             Error = \p^*(x)^2dx-2 \sum_l p_l \frac{N_l}{N} + \sum_l p_l^2 \Delta x
           \end{equation*}
           \begin{equation*}
             \hat{p_l} = \argmin \text{Error} = \argmax_{p_l} \sum_l p_l^2 \Delta x - 2\sum_l p_l \frac{N_l}{N}
           \end{equation*}
           \begin{equation*}
             \frac{\partial}{\partial p_l}... = 2 p_l \Delta x - 2 \frac{N_l}{N} \overset{!}{=} 0
           \end{equation*}
           \begin{equation*}
             p_l = \frac{N_l}{N\Delta x} \quad \text{probability density estimate}
           \end{equation*}
           insert into error:
           \begin{equation*}
             \begin{align*}
               Error &= \int p^*(x)^2dx-2 \sum_l \frac{N_l}{N\Delta x}\frac{N_l}{N} + \sum_l(\frac{N_l}{N\Delta x})^2 \Delta x \\
               &= \int p^*(x)^2dx - \sum_l(\frac{N_l}{N})^2\frac{1}{\Delta x} \\
               &= \int p^*(x)^2dx - \sum_l \hat{p_l}^2 \Delta x
             \end{align*}
           \end{equation*}
           \begin{itemize}
             \item how to choose $\Delta x$: Difficult, rules of thumb:
             \begin{itemize}
               \item Scotts's rule:
               \begin{equation*}
                 \Delta x = \frac{3.5 \sigma}{\sqrt[3]{N}} \quad \text{exactly optimal if $p^*$ is Gaussian)}
               \end{equation*}
               \item Freedman-Diaconis rule:
               \begin{equation*}
                 \Delta x = \frac{2\text{IQR}(x)}{\sqrt[3]{N}}
               \end{equation*}
               IQR: inter-quartile range:
               place data in sorted order: $x_{[1]},x_{[2]},...,x_{[N]}$
               \begin{equation*}
                 IQR = x_{[\frac{3}{4}N]}-x_{[\frac{1}{4}N]}
               \end{equation*}
               \item Shimazaki/Shinomoto rule:
               \begin{equation*}
                 \hat{\Delta x}= \argmax_{\Delta x} \frac{2m-v}{(\Delta x)^2} \quad \text{$m$: mean bin count, $v$: variance}
               \end{equation*}
               \item cross-validation: split into training sets of size N and test sets of size M \\
               for each candidate $\delta x$ compute error
               \begin{equation*}
                 \sum_l \frac{1}{\Delta x}(\frac{N_l}{N}-\frac{M_l}{M})^2
               \end{equation*}
               and choose $\Delta x$ that minimizes error \\
               in general: if $\varphi$ is a hyperparameter (here: $\Delta x$): use Cross Validation
               and grid search
              \end{itemize}
             \item typical bin counts (e.g. Freedman-Diaconis): scale x such that IQR $(x)=\frac{1}{2}$
             \begin{equation*}
               \Rightarrow \Delta x = \frac{1}{\sqrt[3]{N}}
             \end{equation*}
             if data are uniformely distributed:
             \begin{equation*}
               x_{max}-x_{min} = 2 \text{IQR}{x} = 1
             \end{equation*}
             \begin{equation*}
               \text{bin count} \quad \frac{x_{max}-x_{min}}{\Delta x} = \frac{1}{\Delta x} = \sqrt[3]{N}
             \end{equation*}
             \begin{equation*}
               N = 1000 \Rightarrow 10 \text{ bins} \quad N=10^6 \Rightarrow 100 \text{ bins}
             \end{equation*}
             \item generalize to the multi-dimensional case $x \in \mathbb{R}^D$ \\
             naive solution: split each dimension according to Freedman/Diaconis \\
             $\Rightarrow$ 10 bins per dimension $\Rightarrow 10^D$ bins in total \\
             $\Rightarrow$ no TS can ever fill $10^D$ bins $\Rightarrow$ most are empty $\Rightarrow$ no estimate \\
             $\Rightarrow$ doesn't work
           \end{itemize}

             \begin{itemize}
               \item use a 1-dimensional histogram for each dimension (only 10*D bins)
               \item only use 1-D histogramms, one per feature per class
               \item probabalistic interpretation: if we know the class y, all fearture dimension
               are statistically independent
               \begin{equation*}
                 p(x|y)=p(\{ x_j\}|y)=p(x_{j=1}|y)p(x_{j=2}|y)p(x_{j=D}|y)
               \end{equation*}
               \item density trees: place bins adaptively: many bins in subregions with many data bins,
               few bins in subregions with few data points
             \end{itemize}
            \subsubsection{Naive Bayes}
              Using one histogramm per dimension $\iff$ assumption that values of different features are independent,
              given the class $\iff$ if we know the class label $y_i$, then knowing the feature $x_{i1}$ doesn't tell us anything about other feature values $x_{ij} \quad j\neq 1$ \\
              This assumption is often violated, e.g. in images. Consider an image region with class
              label 'sky'. Let one pixel in the sky have color 'blue'. Than it's likely that the neighbor pixels
              are probably also 'blue'. The same applies to other colors (e.g. grey for clouds or red for sunset).
              If assumption is true, joint probability
              \begin{equation*}
                p(x_i=[x_{i1},...,x_{iD}]|y_i) = \Pi_{j=1}^{D}p_j(x_{ij}|y_i)
              \end{equation*}
              \begin{equation*}
                \text{Bayes:} \quad p(y_i|x_i) = \frac{p(x_i|y_i)p(y_i)}{p(x_i)} \overset{\text{naive}}{=}
                \frac{\Pi_{j=1}^D p_j(x_{ij}|y_i)p(y_i)}{p(x_i)}
              \end{equation*}
              \begin{equation*}
                \text{simplify} \quad p(y_j) = p(y_{ji})=\frac{1}{C} \quad \text{uniform priors}
              \end{equation*}
              \begin{equation*}
                \begin{align*}
                  \text{decision rule:} \quad \hat{y_i} &= \argmax_k p(y_i=k|x_i) \\
                  &= \argmax_k \Pi_{j=1}^D p_j(x_{ij}|y_i=k) \\
                  &= \argmax_k \sum_{j=1}^D \log p_j(x_{ij}|y_i=k)
                \end{align*}
              \end{equation*}
              training: for $k=1,...C$ for $j=1,...,D$: fit 1-D histogram $p_j(x_{ij}|y_i=k)$ using the
              j-th feature of all instances of class k

              \paragraph{Variant:} Instead of 1-D histogramms, fit 1-D Gaussian distributions
              $\Rightarrow$ naive BAyes becomes equivalent to QDA with constraint that covariance matrices
              $\Sigma_k$ are all diagonal $\iff$ axes of ellipses are parallel to coordinate axes.
          \subsubsection{Density trees}
            \begin{itemize}
              \item idea: place bins adaptively, small bins where there are many datapoints,
              big bins where there are few
              \item how to find the bin for a data point efficently:
              \begin{equation*}
                1-D \quad l=\floor*{\frac{x-x_{min}}{\Delta x}}}
              \end{equation*}
              \item higher dimensions: represent binning by a binary tree \\
              \begin{forest}[$x_{i1} < 0.4$ ?[$x_{i2} < 0.7$ ?[bin 1][bin 2]][$x_{i2} < 0.7$ ?[bin 3][bin 4]]]
              \end{forest}
              \item bin index for L bins can be found with $\mathcal{O}(\log L)$ decisions (if tree balanced) - 'recursive subdivision'
              \item training
              \begin{itemize}
                \item build the tree
                \item define response ($\hat{p_l}$) of the leaves, e.g. as in 1-D histograms
                \begin{equation*}
                  \hat{p_l} = \frac{N_l}{N V_l} \quad \text{$N_l$ # instances in bin l, $V_l$ volume of bin l}
                \end{equation*}
                or: fit Gaussian to each bin (adjust normalization for finite bin size)
              \end{itemize}
            \end{itemize}
            Build tree by 'recursive best-first expansion' \\
            Init:
            \begin{itemize}
              \item put all data into a single bin (root of tree), size: bounding rectangle of data points
              \item fix #bins $L=\tau \sqrt[3]{N}$
              \item for $t=1,...,L-1$
              \begin{itemize}
                \item compute 'score' of all current leave modes
                \item split best leaf into two children (original leaf is now an interior node)
              \end{itemize}
            \end{itemize}
            score of a leaf: maximal improvement of our objective funtion (e.g. error) if we would split this leaf)
            \begin{itemize}
              \item Criminisi et al.: try a number of random splits and remember the best (allow oblique splits)
              \item exhaustive search for best split (preferable when we split axis orthogonal)
            \end{itemize}
            \paragraph{exhaustive search:} give leaf with boundaries $x_j \in [m_j, M_j]$, $N_l =$ # instances in this leaf \\
              for each feature $j \in 1,...,D$:
              \begin{itemize}
                \item define candidate split thresholds
                \begin{equation*}
                  s_j = \{ s_{ja},...,s_{j(N_l+1)} \}: \quad \text{sort data according to feature j}
                \end{equation*}
                \begin{equation*}
                  x_{[0]j} = m_j = x_{[1]j} \leq x_{[2]j} \leq ... \leq x_{[N_l]j} \leq M_j = X_{[N_l+1]j}
                \end{equation*}
                place candidate threshold in the middle of each pair
                \item compute the score of every candidate split
                return dimension j and threshold $s_{ja}$ and score $g_{ja}$ of best candidate split (among $D(N_l+1)$)
              \end{itemize}
              scores:
              \begin{itemize}
                \item minimize squared error of histogram:
                \begin{equation*}
                  \begin{align*}
                  \text{error} &= \int p^*(x)^2dx - \sum_l^{L_t} \hat{p_l}^2 V_l \quad \text{before split} \\
                  \text{error'} &= \int p^*(x)^2dx - \sum_{l=1}^{L_t+1} \hat{p_l'}^2 V_l
                  \end{align*}
                \end{equation*}
                suppose split leaf l into $\lambda$ and $\rho$
                \begin{equation*}
                  \text{gain } g = \text{error}-\text{error'} = -\hat{p_l}^2V_l+\hat{p_{\lambda}}^2 V_{\lambda}+\hat{p_{\rho}}^2 V_{\rho}
                \end{equation*}
                \item split nodes where data distribution is far from uniform $\iff \frac{N_l}{N} \sim V_l$
                \begin{equation*}
                  \text{non-uniformity}: \abs*{\frac{N_{\lambda}}{N_l} - \frac{V_{\lambda}}{V_l}}
                \end{equation*}
                \item split to minimize etropy (ex. fit Gaussian)
                \begin{equation*}
                  H = \frac{1}{2} \log(\text{det}(2\pi e \Sigma))
                \end{equation*}
                \begin{equation*}
                  g = H_l - \frac{N_{\lambda}}{N}H_{\lambda}-\frac{N_{\rho}}{N}H_{\rho}
                \end{equation*}
              \end{itemize}
              density trees tend to overfit:
              \begin{itemize}
                \item traditional: pruning $\widehat{=}$ cut-off subtrees with high overfitting and replace by single leaf
                \item modern: density forest $\widehat{=}$ train many trees and take their average probability ('ensemble method' $\Rightarrow$ later) \\
                how to make the trees different:
                \begin{enumerate}
                  \item train every tree on a random subset of the training data (bootstrap sampling)
                  \item consider a random subset of the features inthe split: search algorithm
                \end{enumerate}
              \end{itemize}
  \section{Regression}
    \subsection{Introduction}
      \begin{itemize}
        \item Learn model $y = f(x) \quad x \in \mathbb{R}^D \Rightarrow y \in \mathbbm{R}^{D'} (D'=1)$
        \item training set $\{ (\underbrace{x_i}_{\substack{\text{D-dim} \\ \text{features}}},
        \underbrace{y_i}_{\substack{
        \text{real-valued} \\ \text{true response}}}) \}_{i=1}^N \quad \text{assume i.i.d.} $
        - matrix notation
      \end{itemize}
      \begin{tikzpicture}[sibling distance=2cm,level distance=3.5cm,
        box/.style={
             shape=rectangle,
             font=\small,
             draw,
             align=center,
             minimum height=1.5cm,
             text width=1.5cm,
             top color=white,
             bottom color=white}
      ]
      \node  [box,text width=3cm] [sibling distance=8cm] {model class $f(X;\Theta)$} [edge from parent fork down]
        child [sibling distance=7cm] { node [box] {linear}
             child { node [box] {Gaussian additive noise}
                  child [sibling distance=2.5cm]{ node [box] {Is only Y noisy?}
                        child [sibling distance=2cm]{ node [box] {Yes: Is model under-deter-mined D>>N ?}
                              child [sibling distance=2cm] { node [box] {No: Have all $y_i$ same noise variance?}
                                    child [sibling distance=2cm] { node [box] {No: weighted least squares}}
                                    child { node [box] {Yes: ordinary least squares OLS}}}
                              child { node [box] {Yes: regularized, constrained least squares}}}
                        child { node [box] {No: total least squares errors in variance model}}}}
             child [sibling distance=2cm]{ node [box] {Non-Gaussian noise}
                  child { node [box] {Poisson noise}
                        child { node [box] {transform additive Gaussian noise via Anscombe transform }}}
                  child { node [box] {Other known noise model}
                        child { node [box] {Maximum likelihood estimation}}}}}
        child [sibling distance=5cm] { node [box] (lat) {non-linear}
              child [sibling distance=4cm]  { node [box] {Gaussian additive noise?}
                   child [sibling distance=2cm] { node [box] {Yes: non linear least-squares}
                        child { node [box] {neural networks with squared loss function}}}
                   child [sibling distance=2cm] { node [box] {No: linear regression in non-linearity feature space}
                        child { node [box] {Kernel regression}}}
                   child [sibling distance=2cm] { node [box] {No/don't care: regression neural networks}}}};
      \end{tikzpicture}
    \subsection{Ordinary Least Squares (OLS)}
      \begin{itemize}
        \item linear model with additive Gaussian noise, X is noise-free,
        all Y have same noise variance
        \begin{equation*}
          Y = \underbrace{X}_{\substack{\text{Row vector} \\
          \text{D-dim} \\ \text{feature}}} \cdot \underbrace
          {\beta}_{\substack{\text{column vector} \\
          \text{of D-dim weights} \\ \text{activations}}} +
          \underbrace{\varepsilon}_{\substack{\text{Gaussian} \\
          \text{distributed} \\ \text{scalar}}}
        \end{equation*}
        \begin{equation*}
          \varepsilon \sim N(0, \sigma^2) \iff Y \sim N(X\beta, \sigma^2)
        \end{equation*}
        \item find best $\hat{\beta}$ via Maximum Likelihood principle $\hat{=}$
        make training set typical under the model \\
        compute the residuals
        \begin{equation*}
          r_i = y_i - x_i\beta
        \end{equation*}
        \begin{equation*}
          \text{If the model is correct } (\hat{\beta}=\beta^*):\quad r_i \sim N(0, \sigma^2)
        \end{equation*}
        If model is correct and we know the truth:  $\beta = \beta^* \rightarrow r_i =
        \varepsilon_i$, we only have $\beta = \hat{\beta} \rightarrow r_i \sim \varepsilon_i$
        \item ML principle
        \begin{equation*}
          \begin{align*}
            \hat{\beta}&= \argmax_{\beta} p(\{ y_1,...,y_N\} |\{ x_1,...,x_N\};\beta) \\
            &= \argmax_{\beta} \prod_{i=1}^{N} \underbrace{p(y_i|x_i;\beta)}_{
            y*e^{\frac{-(y_i-x_i\beta)^2}{2\sigma^2}}}
             \\
            &= \argmin_{\beta} - \sum_{i=1}^N \log p(y_i|x_i; \beta) \\
            &= \argmin_{\beta} \sum_{i=1}^N \frac{(y_i-x_i\beta)^2}{2\sigma^2} -
            N \log v \\
            &= \argmin_{\beta} \sum_{i=1}^N (y_i-x_i\beta)^2 \quad \text{least-squares objective}
          \end{align*}
        \end{equation*}
        \item example: fit a line in 2D $X \in \mathbb{R}$
        \begin{equation*}
          Y = [X;1]
          \left[ \begin{array}{c}
          a \\
          b
          \end{array}\right] + \varepsilon
        \end{equation*}
        \begin{equation*}
          Y = aX + b + \varepsilon
        \end{equation*}
        \begin{equation*}
          \hat{a}, \hat{b} = \argmin_{a,b} \underbrace{\sum_{i=1}^N (Y_i-aX_i-b)^2}_{
          \text{Loss}}
        \end{equation*}
        \begin{equation*}
          \frac{\partial \text{Loss}}{\partial b} = \sum \sum_{i=1}^N (Y_i-aX_i-b)
          (-1)\overset{!}{=}0
        \end{equation*}
        \begin{equation*}
          \sum_iY_i-a\sum_iX_i-\sum_ib=0
        \end{equation*}
        \begin{equation*}
          \frac{1}{N}\sum_iY_i=a\frac{1}{N}\sum_iX_i+b
        \end{equation*}
        \begin{equation*}
          \bar{Y} = a\bar{X}+b
        \end{equation*}
        Regressionline always goes through the origin
        $\Rightarrow$ always center data(i.e. $\bar{X}=0, \bar{Y}=0)$
        \begin{itemize}
          \item regression goes through origin $\Rightarrow$ no need for
          intercept b
          \item numbers get smaller $\Rightarrow$ regression is numerically more stable
        \end{itemize}
        $\Rightarrow$ assume $X \Rightarrow X-\bar{X}, Y \Rightarrow Y-\bar{Y}$
      \end{itemize}
      rewrite objective in matrix form
      \begin{equation*}
        \hat{\beta} = \argmin_{\beta}(Y-X\beta)^T(Y-X\beta)
      \end{equation*}
      \begin{equation*}
        \frac{\partial \text{Loss}}{\partial \beta} = 2X^T(Y-X\beta) \overset{!}{=}0
      \end{equation*}
      \begin{equation*}
        \underbrace{X^TX}_{\text{scatter matrix}}\beta = X^TY \quad \text{linear system of equations 'normal equations'}
      \end{equation*}
      $\Rightarrow$ solve for $\beta$ \\
      possibilities to solve:
      \begin{enumerate}
        \item formal solution:
        \begin{equation*}
          \underbrace{(X^TX)^{-1}(X^TX)}_{\mathbbm{1}}\beta = (X^TX)^{-1}X^TY
        \end{equation*}
        $(X^TX)^{-1}$ exists if X has full rank description
        \begin{equation*}
          \hat{\beta}= \underbrace{(X^TX)^{-1}X^T}_{X^+}Y
        \end{equation*}
        \begin{equation*}
          X^+ = (X^TX)^{-1}X^T: \quad \text{Moore-Penrose pseudo-inverse, inverse to rectangular matrices}
        \end{equation*}
        \item Cholesky factorization: $X^TX$ is positive definite symmetric \\
        for every such matrix, there is a decomposition:
        \begin{equation*}
          R^TR=X^TX \quad R:D*D \text{ upper triangular}
        \end{equation*}
        \begin{equation*}
          R^TR\beta = X^TY \quad \text{define } Z=R\beta
        \end{equation*}
        \begin{equation*}
          R^TZ=X^TY=F\quad \text{linear equations in Z}
        \end{equation*}
        \begin{enumerate}
          \item solve for Z by forward substitution because $R^T$ is lower triangular
          \item solve $R\beta=Z$ by backward substitution because R is upper triangular
        \end{enumerate}
        advantages:
        \begin{itemize}
          \item easy to construct: $S=X^TX$, we don't have to construct X first \newline
            for k from 1 to D: \newline
            \-\hspace{1cm}  for l from 1 to D: \newline
            \-\hspace{2cm}      $s_{kl}$ = 0 \newline
            \-\hspace{2cm}     for i from 1 to N: \newline
            \-\hspace{3cm}         $s_{kl} += x_{ik}x_{il}$ \newline
        \end{itemize}
        disadvantages:
        \begin{itemize}
          \item Loss of precision: Condition number of metric X
          \begin{equation*}
            \kappa = \norm{X}_F\norm{X^+}_{F}
          \end{equation*}
          \begin{equation*}
            \text{Frobenius norm:} \quad \norm{X}_F = \sqrt{\sum_{k,l} X_{kl}^2}
          \end{equation*}
          \item condition of $S = X^TX$ \\
          $\Rightarrow$ number of significant digits: $m\beta \approx \kappa^2\varepsilon$ \\
          $\varepsilon: $ machine precision, type double: $10^{-16}$ \\
          \begin{equation*}
            X^+ = (X^TX)^{-1}X^T \quad \text{pseudo inverse}
          \end{equation*}
          $\Rightarrow \kappa = 10^8 \Rightarrow \kappa^2\varepsilon \approx 1$
        \end{itemize}
        \item \textbf{Singular Value Decomposition} (SVD) of X: \\
        Theorem: every matrix can be expressed as
        \begin{equation*}
          \underbrace{X}_{N \times D} = \underbrace{U}_{\substack{N \times D \\ \text{orthogonal}}} \cdot
          \underbrace{\Lambda}_{\substack{D \times D \\ \text{diagonal matrix} \\
          \text{of 'singular values'}}} \cdot \underbrace{V^T}_{\substack{\text{orthogonal} \\ D \times D}}
          \quad \text{orthogonal: } U^{-1} = U^T
        \end{equation*}
        If x is square and symmetric: $X=U \cdot \underbrace{\Lambda}_{Eigenvalues} \cdot U^T$ 'Eigenvaluedecomposition'
        \begin{equation*}
          X^T = (U \cdot \Lambda \cdot V^T)^T = (V^T)^T\cdot\Lambda^T\cdot U^T = V\cdot\Lambda\cdot U^T
        \end{equation*}
        \begin{equation*}
          S = X^T \cdot X = V \cdot \Lambda \cdot \underbrace{U^T}_{U^{-1}} \cdot
          U \cdot \Lambda \cdot V^T = V \cdot \Lambda^2\cdot V^T
        \end{equation*}
        \begin{equation*}
          S^{-1} = (V^T)^{-1} \cdot (\Lambda^2)^{-1}\cdot V^{-1} = V \cdot
          \Lambda^{-2}\cdot V^T
        \end{equation*}
        \begin{equation*}
          \beta = S^{-1} \cdot X^T \cdot Y
        \end{equation*}
        \begin{equation*}
          \begin{align*}
            S^{-1}\cdot X^T &=V\cdot\Lambda^{-2}\cdot\underbrace{V^T\cdot V}_{\mathbbm{1}}
            \cdot\Lambda\cdot U^T \\
            &= V \cdot \Lambda^{-2} \cdot V \cdot U^T \\
            &= V \cdot \Lambda^{-1}\cdot U^T = X^T
          \end{align*}
        \end{equation*}
        \begin{equation*}
          \boxed{\beta = V\cdot\Lambda^{-1}\cdot U^T \cdot Y}
        \end{equation*}
        disadvantage: complicated algorithm to find $U, V, \Lambda$ \\
        advantages: \begin{itemize}
          \item numerically most stable method
          \item also works if X does not have full rank (rank $D^* < D \Rightarrow
          D-D^*$ of the singular values $\Lambda$ are 0) \\
          $\Rightarrow$ drop rows of U, columns of $\Lambda$ and $V^T$ with 0
          singular value
        \end{itemize}
        \item \textbf{QR decomposition} (standard, because good compromise between
        numeric stability and effort) \\
        Theorem: every X has a decomposition
        \begin{equation*}
          X = \underbrace{Q}_{\substack{N \times N \\ \text{orthogonal}}} \cdot
          \underbrace{R}_{\substack{N \times D \\ \text{upper triangular}}}
        \end{equation*}
        \begin{equation*}
          \begin{align*}
            X^T &= R^T \cdot Q^T \\
            X^TX &= R^T\cdot Q^T\cdot Q \cdot R = R^T\cdot R \\
            (X^T \cdot X)^{-1} &= R^{-1}\cdot R^{-T} \\
            (X^T\cdot X)^{-1}\cdot X^T &= R^{-1} \cdot R^{-T} \cdot R^T \cdot Q^T =
            R^{-1} \cdot Q^T \\
            \beta &= R^{-1} \cdot Q^T\cdot Y \iff R\cdot \beta = Q^T \cdot Y
          \end{align*}
        \end{equation*}
        solve by backwards substitution \\
        algorithm: construct R one column at a time, immediatly apply the corresponding
        update to RHS (never construct Q)
        \item if X doesn't fit into memory: LSQR algorithm
        \begin{itemize}
          \item variant of conjugate gradient algorithm to solve linear systems of eg., modified for least squares
          probability
          \item matrix X is only ever accessed via matrix-vector products
          $X \cdot U$ and $X^T \cdot V$ (only vectors M memory) \\
          $\Rightarrow$ pass subroutines for these products to algorithm instead
          of matrix data structure \\
          $\Rightarrow$ algorithm never sees matrix Y and its complicated handling
        \end{itemize}
      \end{enumerate}
      \paragraph{Theorem:} Every X has a decomposition
      \begin{equation*}
        X = \underbrace{U}_{\text{orthogonal}} \cdot \underbrace{B}_
        {\substack{\text{bi-diagonal} \\ \text{matrix}}} \cdot \underbrace{V^T}_
        {\text{bi-diagonal}}
      \end{equation*}
      $\Rightarrow$ solve the linear system by forward substitution
      and each iteration only needs z columns of $\beta$ \\
      $\Rightarrow$ construct $U \cdot B \cdot V^T$ one column/row
      at at time, immediately carry out next itereation of substitution \\
      $\Rightarrow$ one only needs columns/rows t, t-1\\
      $\Rightarrow$ never need full decomposition in memory
      \subsubsection{Computer Topography}
      Consider a single ray from X-ray source
      \begin{equation*}
        I = I_0e^{-\int\mu(x)da}
      \end{equation*}
      goal: use many different directions to find $\mu(x,y)$
      \begin{equation*}
        \boxed{I(b)=I_0e^{-\int \mu(a,b)da}}
      \end{equation*}
      Radon transform of $\mu(a,b)$ \\
      $\Rightarrow$ Task: invert Radon-transformation
      \begin{equation*}
      \log(I(b)) = \log(I_0) - \int \mu(a,b)da
      \end{equation*}
      \begin{equation*}
        Y = \int \mu(a,b)da = -\log \frac{I}{I_0} + \underbrace{\varepsilon}_{Noise}
      \end{equation*}
      Y: response of least squares problem \\
      get X (features) by discretizing the problem; a, b and integral

      \paragraph{Wiederholung Computer Tomography}
      \begin{itemize}
        \item estimate X-ray absorption as a function of location $\mu(a, b)$
        \item measure path integrals:
        \begin{equation*}
          I = I_0 e^{\int_{ray} \mu(a,b) dray}
        \end{equation*}
        \begin{equation*}
          \iff \ln \frac{I_0}{I} = \int_{ray} \mu(a, b) dray
        \end{equation*}
        \item to make this solvable, we need many rays
        \item to make this solvable with least squares, we must discretize
      \end{itemize}
      \paragraph{Step 1} use detector array and $N_p$ parallel rays
      \paragraph{Step 2} use $N_0$ different orientations
      $\Rightarrow$ $\#$ of measurements $N=N_p \cdot N_0$
      \begin{equation*}
        \text{instance index} \quad i = \underbrace{i_p}_{detector index} + N_p
        \cdot \underbrace{i_0}_{angel index}
      \end{equation*}
      \paragraph{Step 3} Discretize $\mu(a,b)$:
      \begin{equation*}
        \begin{align*}
          a &= a_0 + j_a \Delta a \\
          b &= b_0 + j_b \Delta b
        \end{align*}
      \end{equation*}
      unknown:
      \begin{equation*}
        \mu(j_a, j_b) \iff \beta_j \quad \text{with } j = j_a + D_a \cdot j_b \quad
        \text{'flattening of image'}
      \end{equation*}
      \paragraph{Step 4} Discretize Integral: relax $\delta((a,b) \in ray)$ to
      $\Rightarrow$ triangular shape $\iff$ change integral into sum:
      \begin{equation*}
        \begin{align*}
        y_i &= \int \mu(a,b) \delta((a,b) \in ray) da db \\
        &\approx \sum_{j_a, j_b} \underbrace{\mu(j_a, j_b)}_{\beta_j} \cdot
        \underbrace{\text{weight}((j_a, j_b), ray)}_{x_{ij}} +
        \underbrace{\varepsilon_i}_{\text{noise}}
        \end{align*}
      \end{equation*}
      \begin{equation*}
        x_{ij} = \text{triangle} (\text{dist}(a=a_0+j_a \Delta a, b= b_0+j_b \Delta b), ray_i)
      \end{equation*}
      Matrix Notation:
      \begin{equation*}
        Y = X\beta + \varepsilon
      \end{equation*}
      This is LSQ! Solve with your favorite LSQ method
      \subsubsection{Different $Y_i$ have different noise level $\sigma_i$}
      \paragraph{Case 1:} simple analytic formula for $\sigma_i$ or $\sigma_i^2$
      $\Rightarrow$ variance stabilizing transformation (almost equal).
      Example: Anscombe transform: $Y \sim \text{Poisson}(Y^*)$
      \begin{equation*}
        \Rightarrow \text{mean} = Y^*, \text{variance} = Y^*
      \end{equation*}
      Transform:
      \begin{equation*}
        \begin{align*}
        \tilde{y} &= 2\sqrt{y+\frac{3}{8}} \sim \mathcal{N}(2
        \sqrt{y^*+\frac{3}{8}}-\frac{1}{4\sqrt{y^*}}, \sigma^2) \\
        &=1+\underbrace{\mathcal{O}\left(\frac{1}{y^*^4}\right)}_{\text{correction term}}
        \end{align*}
      \end{equation*}
      $\Rightarrow$ apply OLS to $\tilde{y}$ \\
      \paragraph{But:} If y is amplified:
      \begin{equation*}
        y' = ay + b \quad a: \text{amplification factor} \quad b: \text{dark signal}
      \end{equation*}
      Anscomb (y') gives rubbish (not Poisson) \\
      \begin{equation*}
        \Rightarrow \text{apply} \quad \text{Anscomb} \left(\frac{y'-b}{a}\right)
      \end{equation*}
      \paragraph{Case 2} We know $\sigma_i$ for all i:
      \begin{equation*}
        y_i = \left(b \pm \underbrace{0.4}_{\sigma_i}\right) cm
      \end{equation*}
      \begin{equation*}
        \Rightarrow \mathbb{E} \abs{(x_i\beta^+-y_i)^2} = \sigma_i^2 = \mathbbm{E}(r_i^2)
      \end{equation*}
      \begin{equation*}
        \Rightarrow \mathbb{E} \floor*{\frac{r_i^2}{\sigma_i^2}} = 1 = const
      \end{equation*}
      non-parametric stabilization \\
      $\Rightarrow$ weighted least squares:
      \begin{equation*}
        \boxed{\hat{\beta} = \argmin_{\beta} \sum_i \frac{(x_i\beta-y_i)^2}{\sigma_i^2}}
      \end{equation*}
      Matrix notation: covariance matrix:
      \begin{equation*} S =
        \left[
        \begin{array}{ccc}
          \sigma_1^2 & & 0 \\
          & \sigma_2^2 & \\
          0 & & \ddots
        \end{array}
        \right]
      \end{equation*}
      \begin{equation*}
        \begin{align*}
          \hat{\beta} &= \argmin (X\beta-Y)^T S^{-1} (X\beta-Y) \\
          &= \underbrace{(X^TS^{-1}X)^{-1}X^TS^{-1}}_{\text{weighted pseudo-inverse}}Y
        \end{align*}
      \end{equation*}
      aquivalent to change of variables:
      \begin{equation*}
        \begin{align*}
          \tilde{X} &= S^{-\frac{1}{2}}X \\
          \tilde{Y} &= S^{-\frac{1}{2}}Y \\
          S^{-\frac{1}{2}} &= \left[
          \begin{array}{ccc}
            \frac{1}{\sigma_1} & & 0 \\
            & \frac{1}{\sigma_2} & \\
            0 & & \ddots
          \end{array}
          \right] \\
          \beta &= (\tilde{X^T}\tilde{X})^{-1}\tilde{X^T}\tilde{Y}
        \end{align*}
      \end{equation*}
      \paragraph{Case 3:} We don't know $\sigma_i \Rightarrow$ we need additional
      assumptions, e.g. noise independent for all:
      \begin{arrowlist}
        \item cov-matrix s is diagonal, as in case 2
        \item estimate $\sigma_i$ together with $\beta$
        \item \uwave{Iteratively Re-weighted least squares} \\
        \end{arrowlist}
      Mit: $\sigma_i^{(0)} = 1$ \\ \\
      for $t=1,...,T_{max}$: \\ \\
      \-\hspace{1cm} $\hat{\beta}^{(t)} = \argmin_{\beta} \sum_i
      \frac{(x_i\beta-y_i)^2}{(\sigma_i^{(t-1)})^2}$    \\
      \-\hspace{1cm} $\sigma_i^{(t)} = \abs{x_i\hat{\beta}^{(t)}-y_i}$ \\ \\
      in the limit $N \to \infty$ \\ \\
      \-\hspace{1cm} $\sigma_i^{(2)} = \sigma_i^* \quad$ (ture $\sigma_i$ found) \\ \\
      for finite N (and slight violation of assumption): \\ \\
      \-\hspace{1cm} use a few more iterations
    \subsection{If both Y and X are noisy - Total Least Squares (TLS)} $\hat{=}$
      errors-in-variables-model
      \begin{itemize}
        \item standard interpretation of OLS: $X\beta=Y$ has no solution
          $\Rightarrow$ minimize difference between LHS ans RHS $(X\beta-Y)^2$
        \item alternative interpretation of OLS:
        \begin{itemize}
          \item find $\tilde{Y}$ such that $X\beta=\tilde{Y}$ is solvable
          \item make the difference between Y and $\tilde{Y}=Y+R$ small
          $\Rightarrow$ minimize the correction $\min_R \norm{R}_2^2$
        \end{itemize}
        \item if X is also noisy:
        \begin{itemize}
          \item find $\tilde{X}$ and $\tilde{Y}$ such that $\tilde{X}\beta=\tilde{Y}$
          is solvable
          \item define $\tilde{X}=X+E, \quad \tilde{Y}=Y+R$
          \item define correction matrix $T=[E R]$ $\Rightarrow \min_T \norm{T}_F^2$
        \end{itemize}
      \end{itemize}
      \paragraph{Formal definition of optimization problem}
      \begin{equation*}
        \hat{T}, \hat{\beta} = \argmin_{T, \beta} \norm{T}_F^2 \text{ with } T=[E R]
        \text{ subject to } (X+E)\beta=Y+R
      \end{equation*}
      \paragraph{Theorem: }\\
      Let $Z=[X Y] \leftarrow N \times (D+1)$ \underline{after} centralization of
      X and Y
      \begin{equation*}
        \text{SVD of} \quad Z = \underbrace{U}_{\substack{\text{orthogonal} \\ N\times (D+1)}}
        \cdot \underbrace{\Lambda}_{\substack{\text{diagonal} \\ (D+1)\times(D+1)}} \cdot
        \underbrace{V^T}_{\substack{\text{orthogonal} \\ (D+1)(D+1)}}
      \end{equation*}
      \begin{equation*}
        \left(
        \begin{array}{ccc}
          \lambda_1 & & 0 \\
          & \ddots & \\
          0 & & \lambda_{D+1}
        \end{array}
        \right)
      \end{equation*}
      \begin{equation*}
        \text{Then } \min \norm{T}_F^2 = \min_j(\lambda_j^2)
      \end{equation*}
      \begin{equation*}
        \text{Let }  \hat{j} = \argmin_j \lambda_j^2 \quad \Rightarrow \quad \hat{v} =
        v_{\hat{j}} \text{of matrix V}
      \end{equation*}
      \begin{equation*}
        \text{corresponding singular vector} \quad \hat{v} = \left[
        \begin{array}{c}
          w \\
          \alpha
        \end{array}
        \right]
      \end{equation*}
      \begin{equation*}
        \Rightarrow \quad \boxed{\hat{\beta} = -\frac{w}{\alpha}}
      \end{equation*}
    \subsection{Total Least Squares}
    If Y and X are noisy:
    \begin{itemize}
      \item find corrections $\tilde{X}=X+E, \tilde{Y}=Y+R$ such that $\tilde{X}
      \beta=\tilde{Y}$ is solvable
      \item minimize corrections
      \item algorithm:
      \begin{enumerate}
        \item centralize X and Y
        \item concatenate $[X Y]=Z$
        \item find the singular value of Z with \underline{smallest magnitude}
        and corresponding singular vector $\hat{v}$
        \item
        \begin{equation*}
          \hat{\beta}=-\frac{\hat{v}_{1...D}} {v_{1+D}} \quad
          \hat{v} =
          \left[ \begin{array}{c}
            \hat{v}_{1...D} \\
            v_{1+D}
          \end{array}
          \right]
        \end{equation*}
      \end{enumerate}
      \end{itemize}
      \paragraph{OLS:} Only Y noisy $\Rightarrow$ correct only Y (move values
      in Y-directions)
      \paragraph{TLS:} X and Y must be corrected (move points in X,Y direction).
      Correction must be perpendicular to regression line $\Rightarrow$ best
      correction direction is the smallest singular vector
      \paragraph{Intuitive Interpretation:} Fit the data cloud with an ellipsoid
      (as in QDA) defined by the SVD (singular vector decomposition) of Z (or
      eigendecomposition of $Z^TZ$) $\Rightarrow$ correction direction $\widehat{=}$
      \underlined{normal} of regression hyperplane $\widehat{=}$ smallest ellipsoid radius
    \subsection{The linear system is underdetermined $\Rightarrow$ Regularization}
    What is underdetermined?
    \begin{equation*}
      \beta \in \mathbb{R}^D \Rightarrow \text{want to find D unknown numbers}
    \end{equation*}
    But the data doesn't have enough information to find D unknowns
    \paragraph{Case 1:} $X \in \mathbb{R}^{N\times D} \text{ with } N<D$
    \begin{equation*}
      \text{In general:} \quad \rank(X) \leq \min(N,D)=N<D
    \end{equation*}
    Rank measures the amount of information contained in X
    \paragraph{Case 2:} $D\geq N$, but $\kappa(X)$ (condition number) is bad
    $\widehat{=}$ several columns of X are almost linearly dependent $\widehat{=}
    $ \underline{effective} rank $D^* <D$, $D^* \widehat{=}$ number of singular
    values of X that are significantly different from zero $\widehat{=}D-D^*$
    singular calues are effectively 0 \\
    $\Rightarrow$ we need additional information to compute D numbers $\Rightarrow$
    refularization provides this info
    \subsubsection{Bias-Variance Trade-Off:} \textbf{Bias} (systematic errors): intrinsic error of
    the model, doesn't disappear for $N \to \infty$. \textbf{Variance} (random errors):
    error caused by finite N \\
    \underline{\textbf{Intuition:}} \begin{itemize}
      \item modeling & training effort is mose efficiently spend when both
      errors are about equal
      \item let $TS_N$ be a training set of size N.
      \begin{equation*}
        \text{If data are i.i.d. :} \quad p(TS_N) = \prod_i p(X_i, Y_i)=p(X,Y)^N
      \end{equation*}
      \item let $\beta^*$ be the (unknown) optimal model parametes, $\hat{\beta}$
      solution from $TS_N$
      \item suppose we can repeat experiment with many random $TS_N$
      \item determine the expected squared error of $\hat{\beta}$:
      \begin{equation*}
         \mathbb{E}_{TS_N} [(\hat{\beta}-\beta^*)^2] \quad \text{'Mean
        squared error (MSE) of $\hat{\beta}$'}
      \end{equation*}
    \end{itemize}
    \begin{equation*}
      \begin{align*}
        \mathbb{E}_{TS_N}[(\hat{\beta}-\beta^*)^2] &= \mathbb{E}[((\hat{\beta}-\mathbb{E}
        [\hat{\beta}])^2+(\mathbb{E}[\hat{\beta}]-\beta^*))^2] \\
        &= \underbrace{\mathbb{E}[(\hat{\beta}-\mathbb{E}[\hat{\beta}])^2]}_{
        \text{variance of $\hat{\beta}$}}+\underbrace{\mathbb{E}[(\mathbb{E}[\hat{\beta}]-\beta^*)^2]}_{
        \text{squared bias of $\hat{\beta}$}} + 2\mathbb{E}[(\hat{\beta}-\mathbb{E}
        [\hat{\beta}])(\mathbb{E}[\hat{\beta}]-\beta^*)] \\
        \mathbb{E}[(\hat{\beta}-\mathbb{E}
        [\hat{\beta}])(\mathbb{E}[\hat{\beta}]-\beta^*)]
        &= \mathbb{E}[\hat{\beta}](\mathbb{E}[\hat{\beta}]-\beta^*)-\underbrace
        {\mathbb{E}[\mathbb{E}[\hat{\beta}]]}_{\mathbb{E}[\hat{\beta}]}(\mathbb
        {E}[\hat{\beta}]-\beta^*) = 0
      \end{align*}
    \end{equation*}
    \begin{equation*}
      \Rightarrow \boxed{\mathbb{E}[(\hat{\beta}-\beta^*)^2] = \mathbb{E}[(
      \hat{\beta}-\mathbb{E}[\hat{\beta}])^2]-\mathbb{E}[(\mathbb{E}
      [\hat{\beta}]-\beta^*)^2]}
    \end{equation*}
    \paragraph{Application of OLS:}
    \begin{equation*}
      Y=X\beta+\varepsilon
    \end{equation*}
    \begin{equation*}
      \begin{align*}
        \mathbb{E}[\hat{\beta}]&=\mathbb{E}[(X^TX)^{-1}X^TY] \\
        &= \underbrace{\mathbb{E}[\underbrace{(X^TX)^{-1}X^TX}_{\mathbbm{1}}\beta^*]}
        _{\mathbb{E}(\beta^*)=\beta^*}+\underbrace{\mathbb{E}[(X^TX)^{-1}X^T\varepsilon]}
        _{=(X^TX)^{-1}\underbrace{\mathbb{E}[\varepsilon]}_{0}} \\
        &= \beta^*
      \end{align*}
    \end{equation*}
    \begin{equation*}
      \Rightarrow \mathbb{E}[(\mathbb{E}[\hat{\beta}]-\beta^*)^2]=\mathbb{E}[(
      \beta^*-\beta^*)^2]=0
    \end{equation*}
    $\Rightarrow$ OLS is unbiased. Covariance matrix of $\beta$:
    \begin{equation*}
      \begin{align*}
        \mathbb{E}[(\hat{\beta}-\mathbb{E}[\hat{\beta}])(\hat{\beta}-\mathbb{E}
        [\hat{\beta}])^T] &= \mathbb{E}[(\hat{\beta}-\beta^*)(\hat{\beta}-\beta^*)^T] \\
        &= \mathbb{E}[(X^TX)^{-1}X^T\varepsilon \varepsilon^TX(X^TX)^{-1}] \\
        &= (X^TX)^{-1}X^T \underbrace{\mathbb{E}[\varepsilon \varepsilon^T]}_{\substack{\text
        {covariance of noise} \\ =\sigma^2 \mathbbm{1}}}X(X^TX)^{-1} \\
        &= \sigma^2 \underbrace{(X^TX)^{-1}X^TX}_{\mathbbm{1}}(X^TX)^{-1}
      \end{align*}
    \end{equation*}
    \begin{equation*}
      \Rightarrow \boxed{\mathbb{E}[(\hat{\beta}-\beta^*)(\hat{\beta}-\beta^*)^T]
      =\sigma^2(X^TX)^{-1}}
    \end{equation*}
    This is the error propagation formula from Y to $\hat{\beta}$. We used:
    \begin{equation*}
      \begin{align*}
        \hat{\beta} &=(X^TX)^{-1}X^T(X\beta^*+\varepsilon) \\
        \beta^* &= (X^TX)^{-1}X^TX\beta^* \\
        \hat{\beta}-\beta^* &= (X^TX)^{-1}X^T\varepsilon
      \end{align*}
    \end{equation*}
    $\varepsilon$ $\widehat{=}$ vector of independent zero-mean Gaussian noise values
    with variance $\sigma^2$
    \[ \Rightarrow \hat{\beta} \sim \mathcal{N}(\beta^*,\sigma^2(XX^T)^{-1})\]
    $\Rightarrow \hat{\beta}$ of OLS has high variance when codition $\kappa(X)$
    is bad (effective rank of X is < D)
    \begin{itemize}
      \item If X has bad condition, two (one more) columns are almost linearly
      dependent, e.g. j and $j'$ \\
      OLS can learn parameters $\beta_j$ and $\beta_{j'}$ such that
      \[\sum_i (X_i\beta_j+X_i\beta_{j'}) = 0\] on the training set but $\beta_j$
      and $\beta_{j'}$ are big numbers that cancel on training set
      \begin{equation*}
        \Rightarrow \sum_i
        (X_i\beta_j+X_i\beta_{j'}) \text{ on \underline{test set} is huge $\Rightarrow$
        overfitting}
      \end{equation*}
      \item Intuition: To reduce overfitting we must prevent big coefficients $\beta_j$
      \item restricting the magnitude of $\beta$ may mean, that $\hat{\beta} \approx
      \beta^*$ is no longer achievable $\Rightarrow$ introduced bias. But if the
      bias is less then variance, this doesn't matter, 'trade-off'
      \item penalize such that both error contributions are roughly equal
      \item only meaningful if the $\beta_j$ have equal importance/scale to make penalties comparable
      \item always standardize X beforehand
      \begin{equation*}
        X_j \to \frac{X_j}{\text{stdDev}[X_j]}
      \end{equation*}
      $\Rightarrow$ each column now has unit variance $\widehat{=}$ comparable scales
      \item combine this with centralization $\Rightarrow$ data have zero mean,
      unit variance (standard data preprocessing)
      \item apply inverse transform to $\beta$ at the end
    \end{itemize}
    Allow some bias to achieve a big reduction in variance, total error reduction
    \subsubsection{Ridge Regression}
    Solve constrained least squares problem:
    \begin{equation*}
      \hat{\beta}=\argmin_\beta (X\beta-Y)^2 \quad s.t. \quad \norm{\beta}_2^2=\beta^T\beta \leq t
    \end{equation*}
    Mathematically aquivalent formulation: Add constraint to the loss via Lagrange multiplier $\tau$
    \begin{equation*}
      \hat{\beta}_\tau = \argmin_\beta \underbrace{(X\beta-Y)^2}_{\text{data term}} +
      \underbrace{\tau \beta^T\beta}_{\text{regularization term}}
    \end{equation*}
    Theorem: For each t from the original problem, there is a $\tau$ such that
    the solutions are the same. There are different solution methods
    \begin{enumerate}
      \item via regularized pseudo-inverse
      \begin{equation*}
        \frac{\partial Loss}{\partial \beta}=2X^T(X\beta-Y)+2\tau\beta\overset{!}{=}0
      \end{equation*}
      \begin{equation*}
        (X^TX+\tau\mathbbm{1})\beta=X^TY
      \end{equation*}
      \begin{equation*}
        \boxed{\hat{\beta}=\underbrace{(X^TX+\tau\mathbbm{1})^{-1}X^T}_{X_\tau^+}Y}
      \end{equation*}
      \begin{equation*}
        X_\tau^+: \quad \text{regularized pseudo-inverse}
      \end{equation*}
      \item via reduction to OLS, define:
      \begin{equation*}
        \tilde{X} = \left[
          \begin{array}{c}
            X \\
            \sqrt{\tau}\mathbbm{1}
          \end{array}
        \right]
        \quad
        \tilde{Y} = \left[
          \begin{array}{c}
            Y \\
            0
          \end{array}
        \right]
      \end{equation*}
      \begin{equation*}
        (\tilde{X}\beta-\tilde{Y})^T(\tilde{X}\beta-\tilde{Y}) =
        (X\beta-Y)^T(X\beta-Y)+\tau \beta^T\beta
      \end{equation*}
      \begin{equation*}
        \Rightarrow \hat{\beta}_\tau= \argmin_\beta (\tilde{X}\beta-Y)^2 \quad
        \text{This is OLS problem}
      \end{equation*}
      Now just use any OLS solver
      \item via Singular Value Decomposition:
      \begin{equation*}
        X=U\Lambda V^T\quad X^T=V \Lambda U^T \quad X^TX=V\Lambda^2 V^T
      \end{equation*}
      \begin{equation*}
        \begin{align*}
        X^TX+\tau\mathbbm{1}&=V\Lambda^2V^T+VV^T\tau\mathbbm{1}VV^T \\
        &= V(\Lambda^2+\tau V^T\mathbbm{1}V)V^T \\
        &= V\underbrace{(\Lambda^2+\tau\mathbbm{1})}_{diag\left( \lambda_j^2+\tau \right) }V^T
        \end{align*}
      \end{equation*}
      \begin{equation*}
        \begin{align*}
          (X^TX+\tau\mathbbm{1})^{-1}=V diag\left(\frac{1}{\lambda_j^2+\tau}\right) V^T
        \end{align*}
      \end{equation*}
      \begin{equation*}
        \begin{align*}
          (X^TX+\tau\mathbbm{1})^{-1}X^T=Vdiag\left(\frac{\lambda_j}{\lambda_j^2+\tau}\right) \cancel{V^TV\Lambda}U^T
        \end{align*}
      \end{equation*}
      \begin{equation*}
        \boxed{(X^TX+\tau\mathbbm{1})^{-1}X^T=Vdiag\left(\frac{\lambda_j}{\lambda_j^2+\tau}\right) U^T=X_\tau^+}
      \end{equation*}
    If $\lambda_j \approx 0: \text{OLS}(\tau=0) \quad \frac{1}{\lambda_j} \approx \infty \Rightarrow
    $ instable solution. \\
     Ridge Regression$(\tau>0) \quad \frac{\lambda_j}{\lambda_j^2+\tau} \approx
    \frac{\lambda_j}{\tau} \approx 0 \Rightarrow$ no exploding singular values, stable
    \item Solution via the dual optimization problem $\Rightarrow$ next week
    \end{enumerate}
    Bayesian interpretation of ridge regression:
    \begin{equation*}
      \text{Let } p(\beta|Y) \text{ be the probability for the solution $\beta$ given data Y}
    \end{equation*}
    We want the maximum likelihood $\hat{\beta}=\argmax_\beta p(\beta|Y)=\argmin_\beta
    -\log p(\beta|Y)$.
    Now we expand p by Bayes theorem:
    \begin{equation*}
      \begin{align*}
        \hat{\beta}&=\argmax_\beta p(\beta|Y) \\
        &=\argmin_\beta -\log p(\beta|Y) \\
        &=\argmin_\beta - \log p(Y|\beta)p(\beta) \\
        &=\argmin_\beta -\log \underbrace{p(Y|\beta^*)}_{
        \mathcal{N}(X\beta^*,\sigma^2)}-\log \underbrace{p(\beta^*)}_{\text{prior belief}} \\
        &= \argmin_\beta (X\beta^*-Y)^2 + const
      \end{align*}
    \end{equation*}
    \begin{equation*}
      \text{We used the uninformative prior:} \quad p(\beta^*)=const \Rightarrow -\log p(\beta^*)=const
    \end{equation*}
    \begin{equation*}
      \text{Gaussian prior:} \quad p(\beta^*)=\mathcal{N}(0, \frac{1}{\tau})
    \end{equation*}
    \begin{equation*}
      \Rightarrow -\log p(\beta^*)= \tau\beta^T\beta+const
    \end{equation*}
    \subsubsection{Sparse Regression}
    We change the regularization term: either use threshhold $\norm{\beta}_p \leq t$ or use
    $+\tau \norm{\beta}_p$ lagrange multiplier:
    \begin{itemize}
      \item $p=2 \Rightarrow$ ridge regression.
      \item $p=1 \Rightarrow \text{L1 regularization} \widehat{=} LASSO$
      \item $p=0$ counting regularization
      \begin{equation*}
        \norm{\beta}_0=\sum_j\mathbbm{1}[\beta_j\neq 0] \quad \text{count non-zero elements}
      \end{equation*}
    \end{itemize}
    Optimizing the problem under the $L_0$ ist NP-hard. The $L_1$ is always sparse
    if t is small enough, because optimum is at the corner of teasible region.
    \paragraph{Approximation algorithm:} Orthogonal Matching Pursuit OMP
    \underline{Init:}
    \begin{itemize}
      \item $X \in \mathbb{R}^{N\cross D}, Y \in \mathbb{R}^N, T:$ threshold for
      number of non-zero elements of $\beta$.
      \item set of active column indices of X: $A^{(0)} = \cancel{O}$
      \item set of inactive column indices of X: $B^{(0)} = \{ j:j=1...D \}$
      \item initial residual: $r^{(0)}=y$
    \end{itemize}
    \underline{Iteration:}
    \begin{itemize}
      \item for $t=1,...,T:$
      \begin{itemize}
        \item find $\hat{j}^{(t)}=\argmax_{j\in B^{(t-1)}} \abs{x_j^T \cdot r^{(t-1)}}$
        \item move $\hat{j}^{(t)}$ from B to A
        \item define active matrix $x^{(t)}$ containing all indices from $A^{(t)}$
        \item solve OLS problem $\hat{\beta}^{(t)}=\argmin_\beta \norm{x^{(t)}\beta-y}$
        \item update residual: $r^{(t)}=y-x^{(t)}\hat{\beta}^{(t)}$
      \end{itemize}
    \end{itemize}
    \paragraph{Application of sparse regression - Topic discovery}
    \begin{itemize}
      \item index i (rows of x) corresponds to words, index j (columns of x)
      corresponds to topicd
      \items each column $x_j$ is word frequency hostogramm for topic j
      $\Rightarrow x_{ij} \widehat{=}$ relative frequency of word in document
      that only talks about topic j
      \item Y: word frequency histogram of new document
      \item task: find out, what topic(s) Y talks about by comparison of word counts
      \item linear model: $Y=X\beta + \varepsilon$
      \item $\beta$ must be positive and sparse
      \item Solution: apply non-negative LASSO ($L_1$ regularization) or non-negative
      orthogonal matching pusuit to get $\beta$, non-zwor elements of $\beta
      \widehat{=}$ topics that best explain Y's word frequencies
      \item Extensions: to learn matrix X ('Dictionary') along with $\beta$ of
      training set, 'dictionary learning'
    \end{itemize}
    \subsection{Non-linear Regression}
    Non-linear model with additive gaussian noise $Y=f(X;\beta)+\varepsilon$
    (linear regression is a special case: $f(X;\beta)=X\beta$). There are two approaches:
    \begin{itemize}
      \item find optimal $\beta$ directly:
      \begin{itemize}
        \item non-linear alg. (Levenberg-Marquort Algorithm)
        \item regression tree/forest
      \end{itemize}
      \item Write: \begin{equation*}
        f(X;\beta)=\phi(X) \cdot \beta \quad \phi(X): \mathbb{R}^D \to \mathbb{R}^{D'}
        \quad \text{non-linear mapping}
    \end{equation*}
    \begin{equation*}
      \tilde{X}=\phi(X) \quad \text{'augmented feature'}
    \end{equation*}
    how to find $\phi$?
    \begin{itemize}
      \item hand crafted
      \item kernel trick/methods
      \item learn $\phi(X;\theta)$, e.g. neural network
    \end{itemize}
    \end{itemize}
    \paragraph{Example 1 - XOR Problem}
    True model: $Y=X_1$ XOR $X_2 \neq [X_1, X_2]\cdot \beta+b$ \\
    Solution: Find new axis:
    \begin{equation*}
      \begin{align*}
        \tilde{X_1} &= 2X_1-1 \in \{ -1,1\} \\
        \tilde{X_2} &= 2X_2-1 \in \-1, 1\} \\
        \tilde{X_3} &= \tilde{X_1} \cdot \tilde{X_2} \\
        Y &= \frac{\tilde{X_3}+1}{2} = \tilde{X}\beta = \left[ \tilde{X_1}, \tilde{X_2}, \tilde{X_3}\right] \cdot
        \left[ \begin{array}{c}
        0 \\
        0 \\
        \frac{1}{2} \\
        \end{array}
        \right] + \frac{1}{2}
      \end{align*}
    \end{equation*}
    \noindent \paragraph{Example 2 - Fitting a circle}
    \begin{equation*}
      \text{model:} \quad y_1 = c+ \left[ \begin{array}{c} cos(\phi_i) \\ sin(\phi_i)
      \end{array} \right] \cdot (r+\varepsilon_i)
    \end{equation*}
    Free variable $\phi$, parameters r (radius) and c (center).
    \begin{equation*}
      \begin{align*}
      \text{Least squares problem:} \quad \hat{c}, \hat{r}&=\argmin_{c,r} \sum_i
      (\norm{y_i-c}-r)^2 \\ &=\argmin_{c,r} \sum_i (\underbrace{\sqrt{(y_{i1}-c_1)^2+(y_
      {i2}-c_2)^2}}_{\text{non-linear}}-r)^2
      \end{align*}
    \end{equation*}
    \underline{Solution 1:} non-linear least squares (homework?)
    \underline{Solution 2:} work in an augmented space by means of algebraic loss
    \begin{equation*}
      \begin{align*}
        \hat{c}, \hat{r}&=\argmin_{c,r} \sum_i (\norm{y_i-c}^2-r^2)^2 \\
        &= \argmin_{c,r} \sum_i ((y_{i1}-c_1)^2+(y_{i2}-c_2)^2-r^2)^2 \\
        &= \argmin_{c,r} \sum_i (y_i^2-2c^T \cdot y_i+c^2-r^2)^2
      \end{align*}
    \end{equation*}
    \begin{equation*}
      \begin{align*}
        \tilde{x_i} &= [y_{i1}, y_{i2},1] \\
        \tilde{y_i} &= y_i^Ty_i \\
        \beta &= [2c_1, 2c_2, r^2-c^Tc]^T \\
        \hat{\beta} &= \argmin_{\beta} \sum_i (\tilde{y_i}-\tilde{x_i}\beta)^2
      \end{align*}
    \end{equation*}
    \begin{equation*}
      \begin{align*}
        \hat{c}_1 = \frac{\hat{\beta}_1}{2} \quad \hat{c}_2 = \frac{\hat{\beta}_2}{2}
        \quad \hat{r}=\sqrt{\hat{\beta}_3+\hat{c}^T\hat{c}}
      \end{align*}
    \end{equation*}
    \subsubsection{Levenberg-Marquart Algorithm}
    \textbf{Problem: } $\hat{\beta}=\argmin_\beta \sum_i (Y_i-f(X_i;\beta))^2$
    has no closed form solution because $f(X_i;\beta)$ is non-linear
    \begin{itemize}
      \item linearize f in the neighborhood of a current guess $\beta^{(t)}$ and
      find an improvement
      \begin{equation*}
        \beta^{(t+1)}=\beta^{(t)}+\Delta^{(t)} \text{by \underline{linear least squares}}
      \end{equation*}
      $\Rightarrow$ Iterate until convergence
      \item Linearization by Taylor expansion:
      \begin{equation*}
        f(X;\beta+\Delta) = f(X;\beta)+\frac{\partial f}{\partial \beta} \bigg \vert_\beta
        \cdot \Delta + \mathcal{O}(\Delta^2)
      \end{equation*}
      \begin{equation*}
        \text{Loss } (\beta+\Delta) \approx \sum_i \underbrace{(Y_i-f(X_i;\beta)}_{\tilde{Y}}-
        \underbrace{\frac{\partial f}{\partial \beta} \bigg \vert_\beta}_{\tilde{X}} \cdot \Delta)^2
      \end{equation*}
    \end{itemize}
    $\Rightarrow$ Gauss-Newton algorithm: \\
    Initialization:
    \begin{itemize}
      \item find initial guess $\beta^{(0)}$
      \item for f=1...T (or until convergence):
      \begin{equation*}
        \hat{Delta}^{(t)} = \argmin_{\Delta} \sum_i (\tilde{Y}_i^{(t)}-\tilde{X}_i^{(t)} \cdot \Delta)^2
      \end{equation*}
      \begin{equation*}
        \text{with } \tilde{Y}_i^{(t)}=Y_i-f(X_i;\beta^{(t-1)}), \tilde{X}^{(t)}=
        \frac{\partial f}{\partial \beta} \bigg \vert_{\beta=\beta^{(t-1)}}
      \end{equation*}
      \begin{equation*}
        \beta^{(t)}=\beta^{(t-1)}+\hat{\Delta}^{(t)}
      \end{equation*}
    \end{itemize}
    \begin{itemize}
      \item Solution via pseudo-inverse
      \begin{equation*}
        \tilde{\Delta}^{(t)}=(\tilde{X}^T\tilde{X})^{-1}\tilde{X}^T\tilde{Y}
      \end{equation*}
      \item problem: Gauss-Newton easily overfitts/$\tilde{X}$ may have bad condition
      $\Rightarrow \Delta^{(t)}$ eplodes (no convergence)
      \item Lovenberg uses ridge regression to regularize
      \begin{equation*}
        \hat{\Delta}^{(t)}=(\tilde{X}^T\tilde{X}+\tau \mathbbm{1})^{-1}\tilde{X}^T\tilde{Y}
      \end{equation*}
      effect if $\tau$ small (relative to $\tilde{X}^T\tilde{X}$) \\
      $\Rightarrow$ behaves like unregularized regression \\
      $\Rightarrow$ fast convergence \\
      If $\tau$ big (relative to $\tilde{X}^T\tilde{X}$)
      \begin{equation*}
        \Rightarrow \tilde{\Delta}^{(t)} \approx \frac{1}{\tau} \tilde{X}^T\tilde{Y}
      \end{equation*}
      $\Rightarrow$ gradient decent, with step size $\frac{1}{\tau}$, slow but numerically stable
    \end{itemize}
























  \end{document}
